{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-klOJ_X4zDC",
    "tags": []
   },
   "source": [
    "# pip package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yI1FdDffSSG4",
    "outputId": "ba3b9e79-4be0-4fa3-acd5-81597df89e4c"
   },
   "outputs": [],
   "source": [
    "# !pip install -r multimodal/env/requirement.txt\n",
    "# !pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
    "# !pip3 install torch torchaudio torchvision torchtext torchdata\n",
    "# !pip install torch_tb_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vViKYaWXZICR",
    "tags": []
   },
   "source": [
    "# The imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T08:33:20.418056Z",
     "start_time": "2024-03-28T08:33:19.730828Z"
    },
    "id": "-2b-4-E2xl2E"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "# from torch.nn import LayerNorm, Linear, Dropout, Softmax\n",
    "from einops import rearrange, repeat\n",
    "import copy\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import re\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "from operator import truediv\n",
    "import math\n",
    "from PIL import Image\n",
    "import time\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.io import loadmat as loadmat\n",
    "from scipy import io\n",
    "import torch.utils.data as dataf\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook,Workbook\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "from matplotlib import colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8JqRzoNz7VFi",
    "outputId": "8d59e85d-e8f2-4658-bbbf-00b73c0ce21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# # 确定自己是否在GPU环境下，如果输出gpu则证明在\n",
    "\n",
    "# 查看显卡驱动\n",
    "# !nvidia-smi\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill 5328\n",
    "# %reload_ext tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /root/autodl-tmp/multimodal/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Choose the param Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # jupyter需要下面一行\n",
    "# sys.argv = ['multimodal.py ']\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--configName\", help=\"sectionName\", type=str,default=\"TEST\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "configName = args.configName\n",
    "\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('paramConfig.ini')\n",
    "\n",
    "section = config[configName] \n",
    "\n",
    "# config of dataset names and sources\n",
    "modalName1 = section[\"modalName1\"]\n",
    "modalName2 = section[\"modalName2\"]\n",
    "datasetName = section[\"datasetName\"]\n",
    "# config of training params\n",
    "patchsize = int(section[\"patchsize\"])\n",
    "batchsize = int(section[\"batchsize\"])\n",
    "testSizeNumber = int(section[\"testSizeNumber\"])\n",
    "EPOCH = int(section[\"EPOCH\"])\n",
    "LR = float(section[\"LR\"])\n",
    "HSIOnly = section.getboolean(\"HSIOnly\")\n",
    "num_workers = int(section[\"num_workers\"])\n",
    "\n",
    "# config of paths\n",
    "checkpointName = section['checkpointName']\n",
    "parent_directory = Path(section[\"parent_directory\"])\n",
    "if section.getboolean(\"self_dataset\"):\n",
    "    datasetPath = parent_directory / \"dataset\" / \"img\"\n",
    "else:\n",
    "    datasetPath = parent_directory / \"dataset\"\n",
    "checkpointPath = parent_directory / \"checkpoint\"\n",
    "resultPath = parent_directory / \"result\"\n",
    "imgPath = parent_directory / \"visresult\"\n",
    "tensorboardPath = parent_directory / \"runs\" / f\"{current_time}_{checkpointName}_{configName}\"\n",
    "checkpointDatasetPath = checkpointPath / datasetName\n",
    "datasetPath.mkdir(parents=True, exist_ok=True)\n",
    "checkpointPath.mkdir(parents=True, exist_ok=True)\n",
    "resultPath.mkdir(parents=True, exist_ok=True)\n",
    "imgPath.mkdir(parents=True, exist_ok=True)\n",
    "checkpointDatasetPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=tensorboardPath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do': 'params',\n",
       " 'cls': 'gapx',\n",
       " 'datasetname': 'Augsburg',\n",
       " 'modalname2': 'DSM',\n",
       " 'patchsize': '7',\n",
       " 'parent_directory': '/root/autodl-tmp/multimodal',\n",
       " 'modalname1': 'HSI',\n",
       " 'batchsize': '695',\n",
       " 'testsizenumber': '9671',\n",
       " 'epoch': '300',\n",
       " 'lr': '2e-4',\n",
       " 'hsionly': 'False',\n",
       " 'checkpointname': 'Minato',\n",
       " 'num_workers': '4',\n",
       " 'token_num': '64',\n",
       " 'token_dim': '64',\n",
       " 'kernel_size': '(3,3,3)',\n",
       " 'padding_size': '(0,1,1)',\n",
       " 'select_token_mode': '11,22,12,21',\n",
       " 'transformer_layer_num': '2',\n",
       " 'pos_emb': 'random',\n",
       " 'loss_clip': 'False',\n",
       " 'loss_mode': 'x',\n",
       " 'dim_feedforward': '2048',\n",
       " 'one_wb': 'False',\n",
       " 'emb_heads': '4',\n",
       " 'emb_heads_dim': '16',\n",
       " 'namda': '5e-2',\n",
       " 'noconv': 'False',\n",
       " 'notrans': 'False',\n",
       " 'nosoftmax': 'False',\n",
       " 'center_patch_size': '1',\n",
       " 'cnn1d_out_dim': '4',\n",
       " 'cnn1d_kernel1': '3',\n",
       " 'cnn1d_kernel2': '3',\n",
       " 'load_model': 'none',\n",
       " 'seed': '42',\n",
       " 'network': 'Minato',\n",
       " 'test_batch': '1000',\n",
       " 'self_dataset': 'False',\n",
       " 'to': 'none',\n",
       " 'configName': 'TEST'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params =  dict(section.items())\n",
    "params['configName']=configName\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLuP4BdxPYy0",
    "tags": []
   },
   "source": [
    "# The DatasetConfig Class Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DatasetConfig:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.classNum = None\n",
    "        self.bandNum1 = None\n",
    "        self.bandNum2 = None\n",
    "        self.shape1 = None\n",
    "        self.shape2 = None\n",
    "        self.path = datasetPath / self.name\n",
    "\n",
    "\n",
    "    def getTeOrTr(self,dataSourceName,TrOrTe=\"Tr\"):\n",
    "        # read the modal data\n",
    "        trPath = self.path / \"{}_{}.mat\".format(dataSourceName,TrOrTe)\n",
    "        trainPatch = loadmat(trPath)['Data'].astype(np.float32)\n",
    "        return trainPatch\n",
    "    def getLabel(self,TrOrTe=\"Tr\"):\n",
    "        # read the modal data\n",
    "        labelPath = self.path / '{}Label.mat'.format(TrOrTe)\n",
    "        label = loadmat(labelPath)['Data']\n",
    "        return label\n",
    "\n",
    "    def getTrainLoader(self,type = \"Tr\",batchsize = batchsize):\n",
    "        # HSI patch\n",
    "        TrainPatch1 = self.getTeOrTr(modalName1,type)#(2832, 11, 11, 144)\n",
    "        TrainPatch1 = torch.from_numpy(TrainPatch1).to(torch.float32)\n",
    "        TrainPatch1 = TrainPatch1.permute(0,3,1,2)\n",
    "        TrainPatch1 = TrainPatch1.reshape(TrainPatch1.shape[0],TrainPatch1.shape[1],-1).to(torch.float32) # 2832, 144, 121\n",
    "\n",
    "        # LIDAR patch\n",
    "        TrainPatch2 = self.getTeOrTr(modalName2,type)#(12197, 11, 11, 144)\n",
    "        TrainPatch2 = torch.from_numpy(TrainPatch2).to(torch.float32)\n",
    "        TrainPatch2 = TrainPatch2.permute(0,3,1,2)\n",
    "        TrainPatch2 = TrainPatch2.reshape(TrainPatch2.shape[0],TrainPatch2.shape[1],-1).to(torch.float32)\n",
    "\n",
    "        # Label\n",
    "        TrainLabel1 = self.getLabel(type)#(2832, 11, 11, 144)\n",
    "        TrainLabel1 = torch.from_numpy(TrainLabel1)-1\n",
    "        TrainLabel1 = TrainLabel1.long()\n",
    "        TrainLabel1 = TrainLabel1.reshape(-1)\n",
    "\n",
    "        print(\"{} {} data shape = {}\".format(modalName1,type,TrainPatch1.shape) )\n",
    "        print(\"{} {} data shape = {}\".format(modalName2,type,TrainPatch2.shape))\n",
    "        print(\"{} label shape = {}\".format(type,TrainLabel1.shape))\n",
    "        dataset = dataf.TensorDataset(TrainPatch1,TrainPatch2, TrainLabel1)\n",
    "        trainLoader = dataf.DataLoader(dataset, batch_size=batchsize, shuffle=True, num_workers= num_workers)\n",
    "        #record class number\n",
    "        if self.classNum==None:\n",
    "            self.classNum = len(np.unique(TrainLabel1))\n",
    "        #record HSI band number\n",
    "        if self.bandNum1==None:\n",
    "            self.bandNum1 = TrainPatch1.shape[1]\n",
    "        #record LIDAR band number\n",
    "        if self.bandNum2==None:\n",
    "            self.bandNum2 = TrainPatch2.shape[1]\n",
    "        #record HSI shape\n",
    "        if self.shape1==None:\n",
    "            self.shape1 = TrainPatch1.shape\n",
    "        #record LiDAR shape\n",
    "        if self.shape2==None:\n",
    "            self.shape2 = TrainPatch2.shape\n",
    "\n",
    "        return trainLoader\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4wqqMbYPJyv",
    "tags": []
   },
   "source": [
    "# Train tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OaGl1VvNPFGa"
   },
   "outputs": [],
   "source": [
    "def getResolution(text):\n",
    "    \"\"\"Matches a string of the format (number, number) and returns a tuple\"\"\"\n",
    "    pattern = r'\\(([0-9]+),([0-9]+),([0-9]+)\\)'\n",
    "    match = re.match(pattern,text)\n",
    "    resolution = tuple((int(x) for x in match.groups()))     \n",
    "    return resolution   \n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc\n",
    "\n",
    "def recordExcel(oa,aa,kappa,other,each_acc):\n",
    "    rows = [oa,aa,kappa,other,current_time,configName,section['network']]\n",
    "    for each in each_acc[0]:\n",
    "        rows.append(each)\n",
    "    \n",
    "    sheetName = datasetName+\"_\"+modalName2 if (modalName2=='MS' or  modalName2=='SAR' or  modalName2=='DSM') else datasetName\n",
    "    # Excel文件路径\n",
    "    excel_file_path = resultPath / \"expRes.xlsx\"\n",
    "    try:\n",
    "        # 打开Excel文件\n",
    "        workbook = load_workbook(excel_file_path)\n",
    "        sheet = workbook[sheetName]\n",
    "    except FileNotFoundError:\n",
    "        workbook = Workbook()\n",
    "        sheet = workbook.active\n",
    "        sheet.title = sheetName\n",
    "        sheet.append([\"OA\",\"AA\",\"Kappa\",\"Other\",\"Time\",\"Config\",\"Network\"])\n",
    "    except KeyError:\n",
    "        sheet = workbook.create_sheet(title=sheetName)\n",
    "        sheet.append([\"OA\",\"AA\",\"Kappa\",\"Other\",\"Time\",\"Config\",\"Network\"])\n",
    "    # 将一行内容追加到工作表\n",
    "    sheet.append(rows)\n",
    "    # 保存修改后的Excel文件\n",
    "    workbook.save(excel_file_path)\n",
    "\n",
    "    print(f\"write finished: {excel_file_path}\")\n",
    "\n",
    "def reports (testloader,model,classNum,name=\"Houston\"):\n",
    "    if name == 'Houston':\n",
    "        target_names = ['Healthy grass', 'Stressed grass', 'Synthetic grass'\n",
    "                        ,'Trees', 'Soil', 'Water',\n",
    "                        'Residential', 'Commercial', 'Road', 'Highway',\n",
    "                        'Railway', 'Parking Lot 1', 'Parking Lot 2', 'Tennis Court',\n",
    "                        'Running Track']\n",
    "    elif name == 'Trento':\n",
    "        target_names = ['Apples','Buildings','Ground','Woods','Vineyard',\n",
    "                        'Roads']\n",
    "    elif name == 'MUUFL' or name == 'MUUFLS' or name == 'MUUFLSR':\n",
    "        target_names = ['Trees','Grass_Pure','Grass_Groundsurface','Dirt_And_Sand', 'Road_Materials','Water',\"Buildings'_Shadow\",\n",
    "                    'Buildings','Sidewalk','Yellow_Curb','ClothPanels']\n",
    "    elif name == 'Augsburg':\n",
    "        target_names =  ['Forest','Residential-Area','Industrail-Area','Low-Plants','Allotment','Commericial-Area','Water']\n",
    "    elif name == 'IP':\n",
    "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "                ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed',\n",
    "                'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "                'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "                'Stone-Steel-Towers']\n",
    "    elif name == 'SA':\n",
    "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
    "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
    "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
    "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
    "    elif name == 'UP':\n",
    "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
    "                        'Self-Blocking Bricks','Shadows']\n",
    "    tar,pre = valid(testloader,model)\n",
    "    print(tar,pre)\n",
    "    oa = accuracy_score(tar, pre)\n",
    "    confusion = confusion_matrix(tar, pre,labels=range(classNum))\n",
    "    print(confusion)\n",
    "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "    kappa = cohen_kappa_score(tar, pre)\n",
    "    print(\"Final result:\")\n",
    "    print(\"OA: {:.4f} | AA: {:.4f} | Kappa: {:.4f}\".format(oa, aa, kappa))\n",
    "    print(each_acc)\n",
    "    print(\"**************************************************\")\n",
    "    return confusion, oa*100, each_acc*100, aa*100, kappa*100\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def save_checkpoint(folder_path,fileName,model):\n",
    "    # save the checkpoint\n",
    "    fileName = folder_path / fileName\n",
    "    torch.save(model.state_dict(),fileName)\n",
    "    return fileName\n",
    "\n",
    "def record_output(oa_ae, aa_ae, kappa_ae, element_acc_ae, path):\n",
    "    f = open(path, 'w')\n",
    "    sentence0 = 'OAs for each iteration are:' + str(oa_ae) + '\\n'\n",
    "    f.write(sentence0)\n",
    "    sentence1 = 'AAs for each iteration are:' + str(aa_ae) + '\\n'\n",
    "    f.write(sentence1)\n",
    "    sentence2 = 'KAPPAs for each iteration are:' + str(kappa_ae) + '\\n' + '\\n'\n",
    "    f.write(sentence2)\n",
    "    sentence2_1 = 'each acc  is: ' + str(element_acc_ae) + '\\n'\n",
    "    f.write(sentence2_1)\n",
    "    sentence3 = 'mean_OA ± std_OA is: ' + str(np.mean(oa_ae)) + ' ± ' + str(np.std(oa_ae)) + '\\n'\n",
    "    f.write(sentence3)\n",
    "    sentence4 = 'mean_AA ± std_AA is: ' + str(np.mean(aa_ae)) + ' ± ' + str(np.std(aa_ae)) + '\\n'\n",
    "    f.write(sentence4)\n",
    "    sentence5 = 'mean_KAPPA ± std_KAPPA is: ' + str(np.mean(kappa_ae)) + ' ± ' + str(np.std(kappa_ae)) + '\\n' + '\\n'\n",
    "    f.write(sentence5)\n",
    "\n",
    "    element_mean = np.mean(element_acc_ae, axis=0)\n",
    "    element_std = np.std(element_acc_ae, axis=0)\n",
    "    sentence8 = \"Mean of all elements in confusion matrix: \" + str(element_mean) + '\\n'\n",
    "    f.write(sentence8)\n",
    "    sentence9 = \"Standard deviation of all elements in confusion matrix: \" + str(element_std) + '\\n' + '\\n'\n",
    "    f.write(sentence9)\n",
    "    element_mean = list(element_mean)\n",
    "    element_mean.extend([np.mean(oa_ae),np.mean(aa_ae),np.mean(kappa_ae)])\n",
    "    element_std = list(element_std)\n",
    "    element_std.extend([np.std(oa_ae),np.std(aa_ae),np.std(kappa_ae)])\n",
    "    sentence10 = \"All values without std: \" + str(element_mean) + '\\n' + '\\n'\n",
    "    f.write(sentence10)\n",
    "    sentence11 = \"All values with std: \"\n",
    "    for i,x in enumerate(element_mean):\n",
    "        sentence11 += str(element_mean[i]) + \" ± \" +  str(element_std[i]) + \", \"\n",
    "    sentence11 += \"\\n\"\n",
    "    f.write(sentence11)\n",
    "    f.write(str(params))\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFT WITH CHANNEL TOKENIZATION\n",
    "\n",
    "from torch.nn import LayerNorm,Linear,Dropout,Softmax\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "def INF(B,H,W):\n",
    "     return -torch.diag(torch.tensor(float(\"inf\")).cuda().repeat(H),0).unsqueeze(0).repeat(B*W,1,1)\n",
    "\n",
    "     \n",
    "class HetConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,padding = None, bias = None,p = 64, g = 64):\n",
    "        super(HetConv, self).__init__()\n",
    "        # Groupwise Convolution\n",
    "        self.gwc = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,groups=g,padding = kernel_size//3, stride = stride)\n",
    "        # Pointwise Convolution\n",
    "        self.pwc = nn.Conv2d(in_channels, out_channels, kernel_size=1,groups=p, stride = stride)\n",
    "    def forward(self, x):\n",
    "        return self.gwc(x) + self.pwc(x)   \n",
    "\n",
    "class MCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.1, proj_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.wq = nn.Linear(head_dim, dim , bias=qkv_bias)\n",
    "        self.wk = nn.Linear(head_dim, dim , bias=qkv_bias)\n",
    "        self.wv = nn.Linear(head_dim, dim , bias=qkv_bias)\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim * num_heads, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        q = self.wq(x[:, 0:1, ...].reshape(B, 1, self.num_heads, C // self.num_heads)).permute(0, 2, 1, 3)  # B1C -> B1H(C/H) -> BH1(C/H)\n",
    "        k = self.wk(x.reshape(B, N, self.num_heads, C // self.num_heads)).permute(0, 2, 1, 3)  # BNC -> BNH(C/H) -> BHN(C/H)\n",
    "        v = self.wv(x.reshape(B, N, self.num_heads, C // self.num_heads)).permute(0, 2, 1, 3)  # BNC -> BNH(C/H) -> BHN(C/H)\n",
    "        attn = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale  # BH1(C/H) @ BH(C/H)N -> BH1N\n",
    "        attn = attn.softmax(dim=-1)\n",
    "#         attn = self.attn_drop(attn)\n",
    "        x = torch.einsum('bhij,bhjd->bhid', attn, v).transpose(1, 2)\n",
    "#         x = (attn @ v).transpose(1, 2)\n",
    "        x = x.reshape(B, 1, C * self.num_heads)   # (BH1N @ BHN(C/H)) -> BH1(C/H) -> B1H(C/H) -> B1C\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(dim, 512)\n",
    "        self.fc2 = Linear(512, dim)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout = Dropout(0.1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = dim\n",
    "        self.attention_norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.ffn = Mlp(dim)\n",
    "#         self.attn = Attention(dim = 64)\n",
    "        self.attn = MCrossAttention(dim = dim)\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x= self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads= 8, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0.1, attn_drop=0.1,\n",
    "                 drop_path=0.1, act_layer=nn.GELU, norm_layer=nn.LayerNorm, has_mlp=False):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(dim, eps=1e-6)\n",
    "        for _ in range(2):\n",
    "            layer = Block(dim)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_block in self.layer:\n",
    "            x= layer_block(x)\n",
    "            \n",
    "        encoded = self.encoder_norm(x)\n",
    "       \n",
    "        \n",
    "\n",
    "        return encoded[:,0]\n",
    "\n",
    "\n",
    "class MFT(nn.Module):\n",
    "    def __init__(self, FM, NC, NCLidar, Classes, HSIOnly,patchsize):\n",
    "        super(MFT, self).__init__()\n",
    "        self.HSIOnly = HSIOnly\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, (9, 3, 3), padding=(0,1,1), stride = 1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            HetConv(8 * (NC - 8), FM*4,\n",
    "                p = 1,\n",
    "                g = (FM*4)//4 if (8 * (NC - 8))%FM == 0 else (FM*4)//8,\n",
    "                   ),\n",
    "            nn.BatchNorm2d(FM*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.last_BandSize = NC//2//2//2\n",
    "        \n",
    "        self.lidarConv = nn.Sequential(\n",
    "                        nn.Conv2d(NCLidar,64,3,1,1),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.GELU()\n",
    "                        )\n",
    "        self.ca = TransformerEncoder(FM*4)\n",
    "        self.out3 = nn.Linear(FM*4 , Classes)\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, 4 + 1, FM*4))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        torch.nn.init.xavier_uniform_(self.out3.weight)\n",
    "        torch.nn.init.normal_(self.out3.bias, std=1e-6)\n",
    "        self.token_wA = nn.Parameter(torch.empty(1, 4, 64),\n",
    "                                     requires_grad=True)  # Tokenization parameters\n",
    "        torch.nn.init.xavier_normal_(self.token_wA)\n",
    "        self.token_wV = nn.Parameter(torch.empty(1, 64, 64),\n",
    "                                     requires_grad=True)  # Tokenization parameters\n",
    "        torch.nn.init.xavier_normal_(self.token_wV)\n",
    "        \n",
    "        self.token_wA_L = nn.Parameter(torch.empty(1, 1, 64),\n",
    "                                     requires_grad=True)  # Tokenization parameters\n",
    "        torch.nn.init.xavier_normal_(self.token_wA_L)\n",
    "        self.token_wV_L = nn.Parameter(torch.empty(1, 64, 64),\n",
    "                                     requires_grad=True)  # Tokenization parameters\n",
    "        torch.nn.init.xavier_normal_(self.token_wV_L)\n",
    "        self.patchsize = patchsize\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.reshape(x1.shape[0],-1,self.patchsize,self.patchsize)\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        x2 = x2.reshape(x2.shape[0],-1,self.patchsize,self.patchsize)\n",
    "        x1 = self.conv5(x1)\n",
    "        x1 = x1.reshape(x1.shape[0],-1,self.patchsize,self.patchsize)\n",
    "        \n",
    "        x1 = self.conv6(x1)\n",
    "        x2 = self.lidarConv(x2)\n",
    "        x2 = x2.reshape(x2.shape[0],-1,self.patchsize**2)\n",
    "        x2 = x2.transpose(-1, -2)\n",
    "        wa_L = self.token_wA_L.expand(x1.shape[0],-1,-1)\n",
    "        wa_L = rearrange(wa_L, 'b h w -> b w h')  # Transpose\n",
    "        A_L = torch.einsum('bij,bjk->bik', x2, wa_L)\n",
    "        A_L = rearrange(A_L, 'b h w -> b w h')  # Transpose\n",
    "        A_L = A_L.softmax(dim=-1)\n",
    "        wv_L = self.token_wV_L.expand(x2.shape[0],-1,-1)\n",
    "        VV_L = torch.einsum('bij,bjk->bik', x2, wv_L)\n",
    "        x2 = torch.einsum('bij,bjk->bik', A_L, VV_L)\n",
    "        x1 = x1.flatten(2)\n",
    "        \n",
    "        x1 = x1.transpose(-1, -2)\n",
    "        wa = self.token_wA.expand(x1.shape[0],-1,-1)\n",
    "        wa = rearrange(wa, 'b h w -> b w h')  # Transpose\n",
    "        A = torch.einsum('bij,bjk->bik', x1, wa)\n",
    "        A = rearrange(A, 'b h w -> b w h')  # Transpose\n",
    "        A = A.softmax(dim=-1)\n",
    "        wv = self.token_wV.expand(x1.shape[0],-1,-1)\n",
    "        VV = torch.einsum('bij,bjk->bik', x1, wv)\n",
    "        T = torch.einsum('bij,bjk->bik', A, VV)\n",
    "        x = torch.cat((x2, T), dim = 1) #[b,n+1,dim]\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        x = self.ca(embeddings)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        out3 = self.out3(x)\n",
    "        return out3\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Minato(nn.Module):\n",
    "    def __init__(self,  Classes,c1,c2):\n",
    "      # x1_shape： B C (P P)\n",
    "        super(Minato, self).__init__()\n",
    "        self.HSIOnly = section.getboolean('HSIOnly')\n",
    "        #临时变量\n",
    "        self.out_channels = 8\n",
    "        self.kernel_size = getResolution(section[\"kernel_size\"])\n",
    "        self.padding_size = getResolution(section[\"padding_size\"])\n",
    "        self.token_num = int(section['token_num']) # token number / 2\n",
    "        self.token_dim = int(section['token_dim'])\n",
    "        self.cnn_out_dim = self.token_dim # origin token dim\n",
    "        self.center_patch_size = int(section['center_patch_size'])\n",
    "        self.cnn1d_out_dim = int(section['cnn1d_out_dim'])\n",
    "        self.cnn1d_kernel1 = int(section['cnn1d_kernel1'])\n",
    "        self.cnn1d_kernel2 = int(section['cnn1d_kernel2'])\n",
    "        \n",
    "        self.patch_size = int(section['patchsize'])\n",
    "        self.all_token_num = self.getTokenNum()\n",
    "        self.attn=None\n",
    "        self.v=None\n",
    "\n",
    "        self.conv3d_features = nn.Sequential(\n",
    "            nn.Conv3d(1, self.out_channels, kernel_size=self.kernel_size,padding = self.padding_size,stride = 1),\n",
    "            nn.BatchNorm3d(self.out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2d_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.out_channels*self.calDim(inn=c1,k=self.kernel_size[0],s=1,p=self.padding_size[0]), out_channels=self.cnn_out_dim, kernel_size=(3, 3)),\n",
    "            nn.BatchNorm2d(self.cnn_out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2d_features_2 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=c2, out_channels=self.cnn_out_dim, kernel_size=(3, 3)),\n",
    "          nn.BatchNorm2d(self.cnn_out_dim),\n",
    "          nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv1d_features_1 = nn.Sequential(\n",
    "          nn.Conv1d(in_channels=self.center_patch_size**2, out_channels=self.cnn1d_out_dim, kernel_size=self.cnn1d_kernel1,padding = 1),\n",
    "          nn.BatchNorm1d(self.cnn1d_out_dim),\n",
    "          nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv1d_features_2 = nn.Sequential(\n",
    "          nn.Conv1d(in_channels=self.center_patch_size**2, out_channels=self.cnn1d_out_dim, kernel_size=self.cnn1d_kernel2,padding =1),\n",
    "          nn.BatchNorm1d(self.cnn1d_out_dim),\n",
    "          nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.wa_1 = nn.Parameter(torch.empty(1, self.token_num, self.cnn_out_dim), requires_grad=True)  # Tokenization parameters\n",
    "        self.wb_1 = nn.Parameter(torch.empty(1, self.token_dim, self.cnn_out_dim), requires_grad=True)  # Tokenization parameters\n",
    "        self.wa_2 = nn.Parameter(torch.empty(1, self.token_num, self.cnn_out_dim), requires_grad=True)  # Tokenization parameters\n",
    "        self.wb_2 = nn.Parameter(torch.empty(1, self.token_dim, self.cnn_out_dim), requires_grad=True)  # Tokenization parameters \n",
    "        self.learnableq1 = nn.Parameter(torch.empty(1, 1, self.cnn_out_dim), requires_grad=True)  # Tokenization parameters\n",
    "        self.learnableq2 = nn.Parameter(torch.empty(1,1, self.cnn_out_dim), requires_grad=True)  # Tokenization parameters\n",
    "        torch.nn.init.xavier_normal_(self.wa_1)\n",
    "        torch.nn.init.xavier_normal_(self.wa_2)\n",
    "        torch.nn.init.xavier_normal_(self.learnableq1)\n",
    "        torch.nn.init.xavier_normal_(self.learnableq2)\n",
    "\n",
    "        if section.getboolean('one_wb'):\n",
    "            torch.nn.init.ones_(self.wb_1)\n",
    "            torch.nn.init.ones_(self.wb_2)\n",
    "        else:\n",
    "            torch.nn.init.xavier_normal_(self.wb_1)\n",
    "            torch.nn.init.xavier_normal_(self.wb_2)\n",
    "        # attention\n",
    "        self.emb_heads=int(section['emb_heads'])\n",
    "        self.emb_heads_dim=int(section['emb_heads_dim'])\n",
    "        if section.getboolean('noconv'):\n",
    "            self.embeddinglinear1 = nn.Linear(c1,self.cnn_out_dim,bias=False)\n",
    "            self.embeddinglinear2 = nn.Linear(c2,self.cnn_out_dim,bias=False)\n",
    "\n",
    "        self.qkvlinear1 = nn.Linear(self.cnn_out_dim,3*self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        self.qkvlinear2 = nn.Linear(self.cnn_out_dim,3*self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        \n",
    "        self.pxlinear1=nn.Linear(self.cnn1d_out_dim*self.calDim(c1,1,self.cnn1d_kernel1,1),self.token_dim,bias=False)\n",
    "        self.pxlinear2=nn.Linear(self.cnn1d_out_dim*self.calDim(c2,1,self.cnn1d_kernel2,1),self.token_dim,bias=False)\n",
    "      \n",
    "        self.qlinear1=nn.Linear(self.token_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        self.qlinear2=nn.Linear(self.token_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "\n",
    "        self.qlinear1_d=nn.Linear(self.cnn_out_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        self.qlinear2_d=nn.Linear(self.cnn_out_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "\n",
    "        self.wp1=nn.Linear((self.patch_size-2)**2,1,bias=False)\n",
    "        self.wp2=nn.Linear((self.patch_size-2)**2,1,bias=False)\n",
    "\n",
    "        self.vlinear1=nn.Linear(self.cnn_out_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        self.vlinear2=nn.Linear(self.cnn_out_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        self.relinear1 = nn.Linear(self.cnn_out_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        self.relinear2 = nn.Linear(self.cnn_out_dim,self.emb_heads*self.emb_heads_dim,bias=False)\n",
    "        # self.onelinear1 = nn.Linear(pp,1,bias=False)\n",
    "        # self.onelinear2 = nn.Linear(pp,1,bias=False)\n",
    "        \n",
    "        self.scale = self.emb_heads_dim ** -0.5\n",
    "        if  section['pos_emb']=='random':\n",
    "            self.pos = nn.Parameter(torch.randn(1, self.all_token_num+1, self.token_dim))\n",
    "        if  section['pos_emb']=='concact':\n",
    "            self.pos = nn.Parameter(torch.randn(1, self.all_token_num+1, self.token_dim))\n",
    "        if section['cls'] == 'cls':\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, self.token_dim))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.token_dim, nhead=8,batch_first=True,dim_feedforward=int(section['dim_feedforward']))\n",
    "        self.transformer =  nn.TransformerEncoder(encoder_layer, num_layers=int(section['transformer_layer_num']),)\n",
    "        self.norm = nn.LayerNorm(self.token_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.token_dim),\n",
    "            nn.Linear(self.token_dim, Classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        p = self.patch_size\n",
    "        cp = self.center_patch_size\n",
    "        \n",
    "        b, c, n = x1.shape\n",
    "        # print(x1.shape,x2.shape)\n",
    "        x1 = rearrange(x1, 'b c (h w) -> b c h w',h=p)\n",
    "        x2 = rearrange(x2, 'b c (h w) -> b c h w',h=p)\n",
    "        \n",
    "        x1p = x1[:,:,p//2-cp//2:p//2+cp//2+1,p//2-cp//2:p//2+cp//2+1]\n",
    "        x2p = x2[:,:,p//2-cp//2:p//2+cp//2+1,p//2-cp//2:p//2+cp//2+1]\n",
    "        x1p = rearrange(x1p, 'b c h w -> b (h w) c') # 1 c1\n",
    "        x2p = rearrange(x2p, 'b c h w -> b (h w) c') # 1 c2\n",
    "        if section.getboolean('noconv'):\n",
    "            x1 = rearrange(x1, 'b c h w -> b (h w) c')\n",
    "            \n",
    "            x1 = self.embeddinglinear1(x1)\n",
    "            \n",
    "            x2 = rearrange(x2, 'b c h w -> b (h w) c')\n",
    "            x2 = self.embeddinglinear2(x2)\n",
    "        else:#conv\n",
    "            # x1 -> conv3d -> conv2d\n",
    "            x1 = rearrange(x1, 'b c h w -> b 1 c h w',h=p)\n",
    "            x1 = self.conv3d_features(x1)\n",
    "            x1 = rearrange(x1, 'b c d h w -> b (c d) h w')\n",
    "            x1 = self.conv2d_features(x1)\n",
    "            x1 = rearrange(x1,'b c h w -> b (h w) c') #x1 tokens | B x1_shape[2] cnn_out_dim\n",
    "\n",
    "            # x2 -> conv2d\n",
    "            x2 = self.conv2d_features_2(x2)\n",
    "            x2 = rearrange(x2, 'b c h w -> b (h w) c')#x2 tokens | B x2_shape[2] cnn_out_dim\n",
    "        if section['to'] == 'scfem':\n",
    "            x = torch.concat((x1,x2), dim=1)\n",
    "            x = self.qlinear1_d(x)\n",
    "            x = self.tokenToCls(x)\n",
    "            return x\n",
    "\n",
    "        if section['cls'] == 'fourgap':\n",
    "            x1 = rearrange(x1,'b h w -> b w h')# B dim tokennumber\n",
    "            x1 = self.gap(x1)\n",
    "            x1 = rearrange(x1,'b h w -> b w h')# B 1 dim\n",
    "            x2 = rearrange(x2,'b h w -> b w h')# B dim tokennumber\n",
    "            x2 = self.gap(x2)\n",
    "            x2 = rearrange(x2,'b h w -> b w h')# B 1 dim\n",
    "        if section.getboolean('nosoftmax'):\n",
    "            x = torch.concat((x1,x2), dim=1)\n",
    "        else:\n",
    "            x = self.getTokens(x1,x2,x1p,x2p)\n",
    "        if section['to'] == 'cpscfem' or  section['to'] == '1drcatm' :\n",
    "            x = self.qlinear1_d(x)\n",
    "            x = self.tokenToCls(x)\n",
    "            return x\n",
    "        if section['cls'] == 'cls':\n",
    "            cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = x.shape[0]) #[b,1,dim]\n",
    "            x = torch.concat((cls_tokens,x), dim=1)\n",
    "        \n",
    "            \n",
    "        self.input_tokens = x[0].unsqueeze(0)\n",
    "        \n",
    "        if  section['pos_emb']!='none':\n",
    "            x = x+self.pos[:,:x.shape[1],:]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        if not section.getboolean('notrans'):\n",
    "            x = self.transformer(x)\n",
    "        self.output_tokens = x[0].unsqueeze(0)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        self.normed_tokens = x[0].unsqueeze(0)\n",
    "        \n",
    "        if section['cls'] == 'cls':\n",
    "            x1,x2,fusion,_ = self.getSplitToken(x[:,1:,:])\n",
    "            x= x[:,0,:].squeeze(1)\n",
    "            l = [x1,x2,fusion]\n",
    "            for i in range(len(l)):\n",
    "                l[i] = self.tokenToCls(l[i])\n",
    "            x1,x2,fusion = l\n",
    "            \n",
    "            return x1,x2,fusion,x\n",
    "        \n",
    "        elif section['cls'] == 'gap':\n",
    "            \n",
    "            \n",
    "            x1,x2,fusion,x = self.getSplitToken(x)\n",
    "            l = [x1,x2,fusion,x]\n",
    "            for i in range(len(l)):\n",
    "                l[i] = self.tokenToCls(l[i])\n",
    "            x1,x2,fusion,x = l\n",
    "\n",
    "            return x1,x2,fusion,x\n",
    "        elif section['cls'] == 'fourgap':\n",
    "            x = self.tokenToCls(x)\n",
    "            return None,None,None,x\n",
    "        elif section['cls'] == 'gapx':\n",
    "            x = self.tokenToCls(x)\n",
    "\n",
    "            return x\n",
    "    \n",
    "    \n",
    "    def calDim(self,inn,p,k,s):\n",
    "        return int((inn+2*p-(k-1)-1)/s+1)\n",
    "    \n",
    "    def linear_attn(self,q,k,v,):\n",
    "        # b d n\n",
    "        attended_values = []\n",
    "        attended_value = None\n",
    "        for i in range(self.emb_heads):\n",
    "            kk = F.softmax(k[\n",
    "                :,\n",
    "                i * self.emb_heads_dim: (i + 1) * self.emb_heads_dim,\n",
    "                :\n",
    "            ], dim=2)\n",
    "            qq = F.softmax(q[\n",
    "                :,\n",
    "                i * self.emb_heads_dim: (i + 1) * self.emb_heads_dim,\n",
    "                :\n",
    "            ], dim=1)\n",
    "            vv = v[\n",
    "                :,\n",
    "                i * self.emb_heads_dim: (i + 1) * self.emb_heads_dim,\n",
    "                :\n",
    "            ]\n",
    "            context = kk @ vv.transpose(1, 2) # d * d\n",
    "            attended_value = (\n",
    "                context.transpose(1, 2) @ qq # b * d * n  \n",
    "            )\n",
    "            attended_values.append(attended_value)\n",
    "        aggregated_values = torch.cat(attended_values, dim=1)\n",
    "        aggregated_values = rearrange(aggregated_values,'b d n -> b n d')\n",
    "        return aggregated_values\n",
    "    \n",
    "    def getTokens(self,x1,x2,x1p,x2p):\n",
    "        select_token_mode = section['select_token_mode']\n",
    "        self.attn=[]\n",
    "        self.v=[]\n",
    "        if section['to'] == 'cpscfem':\n",
    "            q1 = self.conv1d_features_1(x1p) # 1 c ->(1@3 p=1 s=1) -> 1 c1\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')# 1 c1\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q2 = self.conv1d_features_2(x2p) # 1 c ->(1@3 p=1 s=1) -> 1 c1\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')# 1 c1\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            x = torch.concat((x1,x2,q1,q2), dim=1)\n",
    "            return x\n",
    "        if select_token_mode == '11,22,12,21':\n",
    "            # w^T\n",
    "            wa_1 = rearrange(self.wa_1,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_1 = rearrange(self.wb_1,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "            wa_2 = rearrange(self.wa_2,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_2 = rearrange(self.wb_2,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "\n",
    "            #softmax(x1*wa1)T\n",
    "            tmp1 = torch.einsum('bij,bjk->bik', x1, wa_1)# x1*wa1           | B x2_shape[2]    N\n",
    "            tmp1 = self.softmax(tmp1)                    # softmax(x1*wa1)  | B x2_shape[2]  dim\n",
    "            tmp1 = rearrange(tmp1,'b h w -> b w h')      # softmax(x1*wa1)T | B N    x2_shape[2]\n",
    "\n",
    "            #x1wb1\n",
    "            tmp2 = torch.einsum('bij,bjk->bik', x1, wb_1) # x1*wb1          | B x2_shape[2] dim\n",
    "\n",
    "            #softmax(x2*wa2)T\n",
    "            tmp3 = torch.einsum('bij,bjk->bik', x2, wa_2)\n",
    "            tmp3 = self.softmax(tmp3)  \n",
    "            tmp3 = rearrange(tmp3,'b h w -> b w h')\n",
    "\n",
    "            #x2wb2\n",
    "            tmp4 = torch.einsum('bij,bjk->bik', x2, wb_2)\n",
    "\n",
    "\n",
    "            #softmax(x1*wa1)T * x1wb1 [ 1 * 1 ] \n",
    "            x1 = torch.einsum('bij,bjk->bik', tmp1, tmp2) # B N dim\n",
    "            #softmax(x2*wa2)T * x2wb2 [ 2 * 2 ] \n",
    "            x2 = torch.einsum('bij,bjk->bik', tmp3, tmp4) # B N dim\n",
    "            #softmax(x1*wa1)T * x2wb2 [ 1 * 2 ] \n",
    "            tmp1 = torch.einsum('bij,bjk->bik', tmp1, tmp4) # B N dim\n",
    "            #softmax(x2*wa2)T * x1wb1 [ 2 * 1 ] \n",
    "            tmp2 = torch.einsum('bij,bjk->bik', tmp3, tmp2) # B N dim\n",
    "            \n",
    "            x = torch.concat((x1,x2,tmp1,tmp2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == '1,2,12,21':\n",
    "            # w^T\n",
    "            wa_1 = rearrange(self.wa_1,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_1 = rearrange(self.wb_1,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "            wa_2 = rearrange(self.wa_2,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_2 = rearrange(self.wb_2,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "\n",
    "            #softmax(x1*wa1)T\n",
    "            tmp1 = torch.einsum('bij,bjk->bik', x1, wa_1)# x1*wa1           | B x2_shape[2]    N\n",
    "            tmp1 = self.softmax(tmp1)                    # softmax(x1*wa1)  | B x2_shape[2]  dim\n",
    "            tmp1 = rearrange(tmp1,'b h w -> b w h')      # softmax(x1*wa1)T | B N    x2_shape[2]\n",
    "\n",
    "            #x1wb1\n",
    "            tmp2 = torch.einsum('bij,bjk->bik', x1, wb_1) # x1*wb1          | B x2_shape[2] dim\n",
    "\n",
    "            #softmax(x2*wa2)T\n",
    "            tmp3 = torch.einsum('bij,bjk->bik', x2, wa_2)\n",
    "            tmp3 = self.softmax(tmp3)  \n",
    "            tmp3 = rearrange(tmp3,'b h w -> b w h')\n",
    "\n",
    "            #x2wb2\n",
    "            tmp4 = torch.einsum('bij,bjk->bik', x2, wb_2)\n",
    "\n",
    "            #softmax(x1*wa1)T * x2wb2 [ 1 * 2 ] \n",
    "            tmp1 = torch.einsum('bij,bjk->bik', tmp1, tmp4) # B N dim\n",
    "            #softmax(x2*wa2)T * x1wb1 [ 2 * 1 ] \n",
    "            tmp2 = torch.einsum('bij,bjk->bik', tmp3, tmp2) # B N dim\n",
    "            \n",
    "            x = torch.concat((x1,x2,tmp1,tmp2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == '1,2':\n",
    "            x = torch.concat((x1,x2), dim=1)\n",
    "            return x\n",
    "        if select_token_mode == '12,21':\n",
    "            # w^T\n",
    "            wa_1 = rearrange(self.wa_1,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_1 = rearrange(self.wb_1,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "            wa_2 = rearrange(self.wa_2,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_2 = rearrange(self.wb_2,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "\n",
    "            #softmax(x1*wa1)T\n",
    "            tmp1 = torch.einsum('bij,bjk->bik', x1, wa_1)# x1*wa1           | B x2_shape[2]    N\n",
    "            tmp1 = self.softmax(tmp1)                    # softmax(x1*wa1)  | B x2_shape[2]  dim\n",
    "            tmp1 = rearrange(tmp1,'b h w -> b w h')      # softmax(x1*wa1)T | B N    x2_shape[2]\n",
    "\n",
    "            #x1wb1\n",
    "            tmp2 = torch.einsum('bij,bjk->bik', x1, wb_1) # x1*wb1          | B x2_shape[2] dim\n",
    "\n",
    "            #softmax(x2*wa2)T\n",
    "            tmp3 = torch.einsum('bij,bjk->bik', x2, wa_2)\n",
    "            tmp3 = self.softmax(tmp3)  \n",
    "            tmp3 = rearrange(tmp3,'b h w -> b w h')\n",
    "\n",
    "            #x2wb2\n",
    "            tmp4 = torch.einsum('bij,bjk->bik', x2, wb_2)\n",
    "\n",
    "            #softmax(x1*wa1)T * x2wb2 [ 1 * 2 ] \n",
    "            tmp1 = torch.einsum('bij,bjk->bik', tmp1, tmp4) # B N dim\n",
    "            #softmax(x2*wa2)T * x1wb1 [ 2 * 1 ] \n",
    "            tmp2 = torch.einsum('bij,bjk->bik', tmp3, tmp2) # B N dim\n",
    "            \n",
    "            x = torch.concat((tmp1,tmp2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == '11,22':\n",
    "            # w^T\n",
    "            wa_1 = rearrange(self.wa_1,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_1 = rearrange(self.wb_1,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "            wa_2 = rearrange(self.wa_2,'b h w -> b w h')# 1,  CNNdim, N\n",
    "            wb_2 = rearrange(self.wb_2,'b h w -> b w h')# 1,  CNNdim, dim\n",
    "\n",
    "            #softmax(x1*wa1)T\n",
    "            tmp1 = torch.einsum('bij,bjk->bik', x1, wa_1)# x1*wa1           | B x2_shape[2]    N\n",
    "            tmp1 = self.softmax(tmp1)                    # softmax(x1*wa1)  | B x2_shape[2]  dim\n",
    "            tmp1 = rearrange(tmp1,'b h w -> b w h')      # softmax(x1*wa1)T | B N    x2_shape[2]\n",
    "\n",
    "            #x1wb1\n",
    "            tmp2 = torch.einsum('bij,bjk->bik', x1, wb_1) # x1*wb1          | B x2_shape[2] dim\n",
    "\n",
    "            #softmax(x2*wa2)T\n",
    "            tmp3 = torch.einsum('bij,bjk->bik', x2, wa_2)\n",
    "            tmp3 = self.softmax(tmp3)  \n",
    "            tmp3 = rearrange(tmp3,'b h w -> b w h')\n",
    "\n",
    "            #x2wb2\n",
    "            tmp4 = torch.einsum('bij,bjk->bik', x2, wb_2)\n",
    "\n",
    "            #softmax(x1*wa1)T * x1wb1 [ 1 * 1 ] \n",
    "            x1 = torch.einsum('bij,bjk->bik', tmp1, tmp2) # B N dim\n",
    "            #softmax(x2*wa2)T * x2wb2 [ 2 * 2 ] \n",
    "            x2 = torch.einsum('bij,bjk->bik', tmp3, tmp4) # B N dim\n",
    "            \n",
    "            x = torch.concat((x1,x2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'q2k1v1,q1k2v2':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q2k1v1\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q1k2v2\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            x = torch.concat((q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'q2k1v1':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q2k1v1\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            return q2k1v1\n",
    "\n",
    "        if select_token_mode == 'q1k2v2':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k2v2\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            return q1k2v2\n",
    "\n",
    "        if select_token_mode == 'q1k1v1,q2k1v1':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            # q2k1v1\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k1v1), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'q1k1v1,q2k2v2':\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'q1k1v1,q2k2v2,q2k1v1,q1k2v2':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'v1,v2,v1,v2':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "\n",
    "            v1=self.vlinear1(x1)\n",
    "            v1=rearrange(v1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'spectralq':\n",
    "            q1 = self.conv1d_features_1(x1p) # b (h w) c -> \n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            return x\n",
    "        if select_token_mode == 'spectralq_res':\n",
    "            res1 = self.relinear1(x1)\n",
    "            res2 = self.relinear2(x2)\n",
    "            q1 = self.conv1d_features_1(x1p)\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        if select_token_mode == 'linear_attn_spectralq':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "        \n",
    "            q1 = self.conv1d_features_1(x1p)\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n d -> b d n')\n",
    "            \n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2, 'b n d -> b d n')\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv2)\n",
    "            \n",
    "            self.v.append(q1[0])\n",
    "            self.v.append(q2[0])\n",
    "            self.v.append(k1[0])\n",
    "            self.v.append(k2[0])\n",
    "            self.v.append(v1[0])\n",
    "            self.v.append(v2[0])\n",
    "            \n",
    "            q1k1v1 = self.linear_attn(q1,k1,v1)\n",
    "            q2k2v2 = self.linear_attn(q2,k2,v2)\n",
    "            q2k1v1 = self.linear_attn(q2,k1,v1)\n",
    "            q1k2v2 = self.linear_attn(q1,k2,v2)\n",
    "            \n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "    \n",
    "        if select_token_mode == 'linear_attn_spectralq_res':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "            res1 = self.relinear1(x1)\n",
    "            res2 = self.relinear2(x2)\n",
    "            q1 = self.conv1d_features_1(x1p)\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n d -> b d n')\n",
    "            \n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2, 'b n d -> b d n')\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv2)\n",
    "            \n",
    "            self.v.append(q1[0])\n",
    "            self.v.append(q2[0])\n",
    "            self.v.append(k1[0])\n",
    "            self.v.append(k2[0])\n",
    "            self.v.append(v1[0])\n",
    "            self.v.append(v2[0])\n",
    "            \n",
    "            q1k1v1 = self.linear_attn(q1,k1,v1) + res1\n",
    "            q2k2v2 = self.linear_attn(q2,k2,v2) + res2\n",
    "            q2k1v1 = self.linear_attn(q2,k1,v1) + res1\n",
    "            q1k2v2 = self.linear_attn(q1,k2,v2) + res2\n",
    "            \n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'linear_attn':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "        \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv2)\n",
    "            \n",
    "            self.v.append(q1[0])\n",
    "            self.v.append(q2[0])\n",
    "            self.v.append(k1[0])\n",
    "            self.v.append(k2[0])\n",
    "            self.v.append(v1[0])\n",
    "            self.v.append(v2[0])\n",
    "            # q1k1v1\n",
    "            q1k1v1 = self.linear_attn(q1,k1,v1)\n",
    "            q2k2v2 = self.linear_attn(q2,k2,v2)\n",
    "            q2k1v1 = self.linear_attn(q2,k1,v1)\n",
    "            q1k2v2 = self.linear_attn(q1,k2,v2)\n",
    "            \n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'linear_attn_res':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "        \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n d -> b d n'), qkv2)\n",
    "            res1 = self.relinear1(x1)\n",
    "            res2 = self.relinear2(x2)\n",
    "            self.v.append(q1[0])\n",
    "            self.v.append(q2[0])\n",
    "            self.v.append(k1[0])\n",
    "            self.v.append(k2[0])\n",
    "            self.v.append(v1[0])\n",
    "            self.v.append(v2[0])\n",
    "            # q1k1v1\n",
    "            q1k1v1 = self.linear_attn(q1,k1,v1)+res1\n",
    "            q2k2v2 = self.linear_attn(q2,k2,v2)+res2\n",
    "            q2k1v1 = self.linear_attn(q2,k1,v1)+res1\n",
    "            q1k2v2 = self.linear_attn(q1,k2,v2)+res2\n",
    "            \n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'q1k1v1,q2k2v2,q2k1v1,q1k2v2_res':\n",
    "            assert x1.shape[1]==x2.shape[1] , f'x1.shape[1]]={x1.shape[1]},x2.shape[1]={x2.shape[1]} x1!=x2'\n",
    "        \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            res1 = self.relinear1(x1)\n",
    "            res2 = self.relinear2(x2)\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads) + res1\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads) + res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads) + res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads) + res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            \n",
    "            return x\n",
    "        if select_token_mode == 'v1v2':\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, _, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, _, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            \n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            return x\n",
    "        if select_token_mode == 'spectralq_res_one':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n')\n",
    "            res1 = self.gap(x1)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c')\n",
    "            \n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "            \n",
    "            q1 = self.conv1d_features_1(x1p)\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "        if select_token_mode == 'spectralq_res_one_true':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "            \n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "            \n",
    "            q1 = self.conv1d_features_1(x1p) # 1 c ->(1@3 p=1 s=1) -> 1 c1\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')# 1 c1\n",
    "            q1 = self.pxlinear1(q1)\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.pxlinear2(q2)\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "            \n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "            \n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "            \n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "         \n",
    "        if select_token_mode == 'cpscfem-gap':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            q1 = self.qlinear1_d(res1)\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            q2 = self.qlinear2_d(res2)\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "        if select_token_mode == 'cpscfem-wp':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            qq1 = self.wp1(x1) # d 1\n",
    "            qq1 = rearrange(qq1, 'b n c -> b c n') # 1 d\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            qq2 = self.wp2(x2) # d 1\n",
    "            qq2 = rearrange(qq2, 'b n c -> b c n') # 1 d\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            q1 = self.qlinear1_d(qq1)\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            q2 = self.qlinear2_d(qq2)\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "        if select_token_mode == 'cpscfem-learnable':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            q1 = self.qlinear1_d(self.learnableq1)\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            q2 = self.qlinear2_d(self.learnableq2)\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "        if select_token_mode == '1drca-nores':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            q1 = self.conv1d_features_1(x1p) # 1 c ->(1@3 p=1 s=1) -> 1 c1\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')# 1 c1\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2,q2k1v1,q1k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "        if select_token_mode == '1drca-justmsa':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            q1, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            q2, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2), dim=1)\n",
    "\n",
    "            return x\n",
    "        if select_token_mode == '1drca-1122':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            q1 = self.conv1d_features_1(x1p) # 1 c ->(1@3 p=1 s=1) -> 1 c1\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')# 1 c1\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k1v1,q2k2v2), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "        if select_token_mode == '1drca-1221':\n",
    "            x1 = rearrange(x1, 'b n c -> b c n') #  d pp\n",
    "            res1 = self.gap(x1) # d 1\n",
    "            res1 = rearrange(res1, 'b n c -> b c n') # 1 d\n",
    "            # print(\"res1 shape:\",res1.shape)\n",
    "            x1 = rearrange(x1, 'b c n -> b n c') # pp d\n",
    "\n",
    "            x2 = rearrange(x2, 'b n c -> b c n')\n",
    "            res2 = self.gap(x2)\n",
    "            res2 = rearrange(res2, 'b n c -> b c n')\n",
    "            # print(\"res2 shape:\",res2.shape)\n",
    "            x2 = rearrange(x2, 'b c n -> b n c')\n",
    "\n",
    "            q1 = self.conv1d_features_1(x1p) # 1 c ->(1@3 p=1 s=1) -> 1 c1\n",
    "            q1 = rearrange(q1, 'b n c -> b 1 (n c)')# 1 c1\n",
    "            q1 = self.qlinear1(q1) # b 1 dim\n",
    "            q1=rearrange(q1, 'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            q2 = self.conv1d_features_2(x2p)\n",
    "            q2 = rearrange(q2, 'b n c -> b 1 (n c)')\n",
    "            q2 = self.qlinear2(q2) # b 1 dim\n",
    "            q2=rearrange(q2,'b n (h d) -> b h n d', h=self.emb_heads)\n",
    "\n",
    "            qkv1=self.qkvlinear1(x1).chunk(3, dim=-1)\n",
    "            _, k1, v1 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv1)\n",
    "            qkv2=self.qkvlinear2(x2).chunk(3, dim=-1)\n",
    "            _, k2, v2 = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.emb_heads), qkv2)\n",
    "\n",
    "            # q1k1v1\n",
    "            q1 = q1*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k1)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q1k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q1k1v1 = rearrange(q1k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q2k2v2\n",
    "            q2 = q2*self.scale\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k2)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q2k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q2k2v2 = rearrange(q2k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            # q2k1v1\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q2, k1)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v1[0])\n",
    "            q2k1v1 = torch.einsum('bhij,bhjd->bhid', attn, v1)\n",
    "            q2k1v1 = rearrange(q2k1v1, 'b h n d -> b n (h d)', h=self.emb_heads)+res1\n",
    "\n",
    "            # q1k2v2\n",
    "            dots = torch.einsum('bhid,bhjd->bhij', q1, k2)\n",
    "            self.attn.append(attn[0])\n",
    "            self.v.append(v2[0])\n",
    "            q1k2v2 = torch.einsum('bhij,bhjd->bhid', attn, v2)\n",
    "            q1k2v2 = rearrange(q1k2v2, 'b h n d -> b n (h d)', h=self.emb_heads)+res2\n",
    "\n",
    "            x = torch.concat((q1k2v2,q2k1v1), dim=1)\n",
    "            # print(x.shape)\n",
    "            return x\n",
    "    def getTokenNum(self):\n",
    "        select_token_mode = section['select_token_mode']\n",
    "        if select_token_mode == '11,22,12,21':\n",
    "            return self.token_num*4\n",
    "        if select_token_mode == '1,2,12,21':\n",
    "            ps=int(self.patch_size)\n",
    "            self.x1_n = self.calDim(inn=ps,k=self.kernel_size[1],p=self.padding_size[1],s=1)\n",
    "            self.x1_n = self.calDim(inn=self.x1_n,k=3,p=0,s=1)**2\n",
    "            self.x2_n = self.calDim(inn=ps,k=3,p=0,s=1)**2\n",
    "            return self.x1_n+self.x2_n+2*self.token_num\n",
    "        if select_token_mode == '1,2':\n",
    "            ps=int(self.patch_size)\n",
    "            self.x1_n = self.calDim(inn=ps,k=self.kernel_size[1],p=self.padding_size[1],s=1)\n",
    "            self.x1_n = self.calDim(inn=self.x1_n,k=3,p=0,s=1)**2\n",
    "            self.x2_n = self.calDim(inn=ps,k=3,p=0,s=1)**2\n",
    "            return self.x1_n+self.x2_n\n",
    "        if select_token_mode == '12,21':\n",
    "            return 2*self.token_num\n",
    "        if select_token_mode == '11,22':\n",
    "            return 2*self.token_num\n",
    "        if select_token_mode == 'q1k1v1,q2k1v1':\n",
    "            ps=int(self.patch_size)\n",
    "            self.x1_n = self.calDim(inn=ps,k=self.kernel_size[1],p=self.padding_size[1],s=1)\n",
    "            self.x1_n = self.calDim(inn=self.x1_n,k=3,p=0,s=1)**2\n",
    "            self.x2_n = self.calDim(inn=ps,k=3,p=0,s=1)**2\n",
    "            return self.x1_n+self.x2_n\n",
    "        if select_token_mode[0]=='q':\n",
    "            ps=int(self.patch_size)\n",
    "            l = select_token_mode.split(',')\n",
    "            self.x1_n = self.calDim(inn=ps,k=self.kernel_size[1],p=self.padding_size[1],s=1)\n",
    "            self.x1_n = self.calDim(inn=self.x1_n,k=3,p=0,s=1)**2\n",
    "            self.x2_n = self.calDim(inn=ps,k=3,p=0,s=1)**2\n",
    "            assert self.x1_n==self.x2_n,\"self.x1_n!=self.x2_n\"\n",
    "            return len(l)*self.x2_n\n",
    "        if select_token_mode=='spectralq_res_one_true':\n",
    "            return 4\n",
    "            \n",
    "    def getSplitToken(self,x):\n",
    "        select_token_mode = section['select_token_mode']\n",
    "        if select_token_mode == '11,22,12,21':\n",
    "            return x[:,0:self.token_num,:],x[:,self.token_num:self.token_num*2,:],x[:,self.token_num*2:,:],x\n",
    "        if select_token_mode == '1,2,12,21':\n",
    "            return x[:,0:self.x1_n,:],x[:,self.x1_n:self.x1_n+self.x2_n,:],x[:,self.x1_n+self.x2_n:,:],x\n",
    "        if select_token_mode == '1,2':\n",
    "            return x[:,0:self.x1_n,:],x[:,self.x1_n:self.x1_n+self.x2_n,:],None,x\n",
    "        if select_token_mode == '12,21':\n",
    "            return None,None,x,x\n",
    "        if select_token_mode == '11,22':\n",
    "            return x[:,0:self.token_num,:],x[:,self.token_num:self.token_num*2,:],None,x\n",
    "        else:\n",
    "            return None,None,None,x\n",
    "        \n",
    "    def tokenToCls(self,x):\n",
    "        if x==None:\n",
    "            return None\n",
    "        # x : B tokennumber dim \n",
    "        x = rearrange(x,'b h w -> b w h')# B dim tokennumber\n",
    "        x = self.gap(x)\n",
    "        x = rearrange(x,'b h w -> b (h w)') # B dim\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# othermethod\n",
    "from models.exvit import MViT as ExViT\n",
    "from models.HCTnet import HCTnet as HCT\n",
    "from models.TDCNN import TDCNN as TDCNN\n",
    "from models.TD1CNN import TD1CNN as TD1CNN\n",
    "from models.TD2CNN import TD2CNN as TD2CNN\n",
    "from models.M2FNet import M2Fnet as M2Fnet\n",
    "\n",
    "def getModel(name,c1,c2,num_classes,patch_size):\n",
    "    if name=='ExViT':\n",
    "        return ExViT(\n",
    "        patch_size = patch_size,\n",
    "        num_patches = [c1,c2],\n",
    "        num_classes = num_classes,\n",
    "        dim = 64,\n",
    "        depth = 6,\n",
    "        heads = 4,\n",
    "        mlp_dim = 16,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1,\n",
    "        mode = 'MViT'\n",
    "    )\n",
    "    elif name=='Minato':\n",
    "        return Minato(c1=c1,\n",
    "        c2=c2,\n",
    "        Classes=num_classes\n",
    "    )\n",
    "    elif name == 'MFT':\n",
    "        return MFT(16, c1,c2, num_classes, False,patch_size)\n",
    "    elif name == 'HCT':\n",
    "        return HCT(c1=c1,\n",
    "                   c2=c2,\n",
    "                   num_classes=num_classes,\n",
    "                   )\n",
    "    elif name == '3DCNN':\n",
    "        return TDCNN(c1=c1,c2=c2,classes=num_classes)\n",
    "    elif name == '1DCNN':\n",
    "        return TD1CNN(c1=c1,c2=c2,classes=num_classes,patchsize=patch_size)\n",
    "    elif name == '2DCNN':\n",
    "        return TD2CNN(c1=c1,c2=c2,classes=num_classes,patchsize=patch_size)\n",
    "    elif name == 'M2Fnet':\n",
    "        return M2Fnet(FM=16, c1=c1, Classes=num_classes,c2=c2,patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0Ei20ufPd98",
    "tags": []
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "Qd1iC--QEobx",
    "outputId": "14e678af-41eb-4775-a605-bb833d4aae1a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------Training for  Augsburg  ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#tool\n",
    "def valid(testLoader,model,is_all=True):\n",
    "    tmp = None\n",
    "    pre = np.array([])\n",
    "    tar = np.array([])\n",
    "    for step, (b_x1, b_x2, b_y) in enumerate(testLoader):\n",
    "        # move train data to GPU\n",
    "        b_x1 = b_x1.cuda()\n",
    "        \n",
    "        if section['cls'] == 'gapx':\n",
    "            if HSIOnly:\n",
    "                    x = model(b_x1,  b_x2)\n",
    "                    p = torch.max(x, 1)[1].squeeze()\n",
    "\n",
    "            else:\n",
    "                    b_x2 = b_x2.cuda()\n",
    "                    x= model(b_x1, b_x2)\n",
    "                    p = torch.max(x, 1)[1].squeeze()\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            if HSIOnly:\n",
    "                    x1,fusion,x = model(b_x1,  b_x2)\n",
    "                    p = torch.max(x, 1)[1].squeeze()\n",
    "\n",
    "            else:\n",
    "                    b_x2 = b_x2.cuda()\n",
    "                    x1,x2,fusion,x= model(b_x1, b_x2)\n",
    "                    p = torch.max(x, 1)[1].squeeze()\n",
    "\n",
    "        tar = np.append(tar, b_y.data.numpy()) # 不断append正确标签集合\n",
    "        pre = np.append(pre, p.data.cpu().numpy()) # 不断append预测标签集合\n",
    "        if not is_all:\n",
    "            if tmp == None:\n",
    "                tmp = 5\n",
    "            tmp-=1\n",
    "            if tmp == 0:\n",
    "                break\n",
    "    return tar,pre\n",
    "\n",
    "\n",
    "def cal_loss(x1,x2,fusion,x,loss_func,y):\n",
    "    namda = float(section['namda'])\n",
    "    if section['loss_mode']=='x':\n",
    "        return loss_func(x,y)\n",
    "    elif section['loss_mode']=='namda(x1,x2,fusion),x':\n",
    "        return namda*(loss_func(x1,y)+loss_func(x2,y)+loss_func(fusion,y))+loss_func(x,y)\n",
    "        \n",
    "print(\"----------------------------------Training for \",datasetName,\" ---------------------------------------------\")\n",
    "def show_image(name,step,data):\n",
    "    w_grid = torchvision.utils.make_grid(data,normalize=True,scale_each=True,padding=2,nrow=int(section['emb_heads']))\n",
    "    writer.add_image(name, w_grid, global_step=step,dataformats =\"CHW\")\n",
    "def train():\n",
    "    datasetConfig = DatasetConfig(datasetName)\n",
    "    trainLoader = datasetConfig.getTrainLoader(type = \"Tr\",batchsize = batchsize)\n",
    "    testLoader = datasetConfig.getTrainLoader(type = \"Te\",batchsize = testSizeNumber)\n",
    "\n",
    "    bestmodel = None\n",
    "\n",
    "    print(\"Number of Classes = \", datasetConfig.classNum)\n",
    "    print(\"Number of band = \", datasetConfig.bandNum1)\n",
    "    KAPPA = []\n",
    "    OA = []\n",
    "    AA = []\n",
    "    ELEMENT_ACC = np.zeros((1, datasetConfig.classNum))\n",
    "    fileName = None\n",
    "\n",
    "    set_seed(int(section['seed']))\n",
    "    # summary(model, [(datasetConfig.bandNum1, patchsize**2),(datasetConfig.bandNum2,patchsize**2)])\n",
    "    for iterNum in range(1):\n",
    "        print(datasetConfig.bandNum1, datasetConfig.bandNum2, datasetConfig.classNum)\n",
    "        if section['load_model']!='none':\n",
    "            print(\"load file:\"+str(checkpointDatasetPath / section['load_model']))\n",
    "            model = torch.load(checkpointDatasetPath / section['load_model'])\n",
    "        else:\n",
    "            model = getModel(section['network'],datasetConfig.bandNum1,datasetConfig.bandNum2,datasetConfig.classNum,int(section['patchsize'])).cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-3)\n",
    "        loss_func = nn.CrossEntropyLoss()  # the target label is not one-hotted\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "        BestAcc = 0\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        # train and test the designed model\n",
    "        for epoch in range(EPOCH):\n",
    "                # 测试模型耗时则取消注释\n",
    "                # prof = torch.profiler.profile(\n",
    "                #   schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "                #   on_trace_ready=torch.profiler.tensorboard_trace_handler(tensorboardPath),\n",
    "                #   record_shapes=True,\n",
    "                #   with_stack=True)\n",
    "                # prof.start()\n",
    "                for step, (b_x1, b_x2, b_y) in enumerate(trainLoader):\n",
    "                        # 测试模型耗时则取消注释\n",
    "                        # prof.step()\n",
    "                        # if step >= 1 + 1 + 3:\n",
    "                        #     print(\"return\")\n",
    "                        #     prof.stop() \n",
    "                        #     return\n",
    "                        # move train data to GPU\n",
    "                        b_x1 = b_x1.cuda()\n",
    "                        b_y = b_y.cuda()\n",
    "                        if section['cls'] == 'gapx':\n",
    "                            if HSIOnly:\n",
    "                                    x = model(b_x1,  b_x2)\n",
    "                                    loss = loss_func(x,b_y)\n",
    "                                    del b_x1,b_y\n",
    "\n",
    "                            else:\n",
    "                                    b_x2 = b_x2.cuda()\n",
    "                                    x= model(b_x1, b_x2)\n",
    "                                    loss = loss_func(x,b_y)\n",
    "                                    del b_x1,b_y,b_x2\n",
    "                        else:\n",
    "                            if HSIOnly:\n",
    "                                    x1,x2,fusion,x = model(b_x1,  b_x2)\n",
    "                                    loss = cal_loss(x1=x1,x2=None,fusion=fusion,x=x,loss_func=loss_func,y = b_y)\n",
    "                                    del b_x1,b_y\n",
    "\n",
    "                            else:\n",
    "                                    b_x2 = b_x2.cuda()\n",
    "                                    x1,x2,fusion,x= model(b_x1, b_x2)\n",
    "                                    loss = cal_loss(x1=x1,x2=x2,fusion=fusion,x=x,loss_func=loss_func,y = b_y)\n",
    "                                    del b_x1,b_y,b_x2\n",
    "                        writer.add_scalar(f\"Loss/train/{iterNum}\", loss, epoch*len(trainLoader)+step)   \n",
    "                        optimizer.zero_grad()  # clear gradients for this training step\n",
    "                        loss.backward()  # backpropagation, compute gradients\n",
    "                        if section.getboolean('loss_clip'):\n",
    "                            nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "                        optimizer.step()  # apply gradients\n",
    "\n",
    "\n",
    "\n",
    "                #每个epoch测一下\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    tar,pre = valid(testLoader,model)\n",
    "                    # accuracy = np.sum(pre==tar) / tar.shape[0]\n",
    "\n",
    "                    oa = accuracy_score(tar, pre)\n",
    "                    confusion = confusion_matrix(tar, pre,labels=range(datasetConfig.classNum))\n",
    "                    # print(confusion)\n",
    "                    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "                    kappa = cohen_kappa_score(tar, pre)\n",
    "\n",
    "\n",
    "                    print(np.sum(pre==tar),tar.shape[0])\n",
    "                    print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.cpu().numpy(), '| test accuracy: %.4f' % (oa*100),'| aa: %.4f' % (aa*100),'| kappa: %.4f' % (kappa*100))\n",
    "                    writer.add_scalars(f\"Epochbased/test/{iterNum}\", {\"loss\":loss.item(),\"test acc\":oa},epoch)\n",
    "\n",
    "                    # draw in tensor\n",
    "                    if section['network']=='Minato':\n",
    "                        if section['select_token_mode'][0]!='1':\n",
    "                            show_image(name=f\"matrix/linear1/{iterNum}\",step=epoch,data=[model.qkvlinear1.weight.detach().cpu()])\n",
    "                            show_image(name=f\"matrix/linear2/{iterNum}\",step=epoch,data=[model.qkvlinear2.weight.detach().cpu()])\n",
    "                            if model.attn!=None and model.attn!=[] :\n",
    "                                att_map=[ i.unsqueeze(0) for i in list(torch.cat(model.attn,dim=0))]\n",
    "                                show_image(name=f\"matrix/att_map/{iterNum}\",step=epoch,data=att_map)\n",
    "                            if model.v!=None and model.v!=[]:\n",
    "                                v=[ i.unsqueeze(0) for i in list(torch.cat(model.v,dim=0))]\n",
    "                                show_image(name=f\"matrix/v/{iterNum}\",step=epoch,data=v)\n",
    "                        else:\n",
    "                            show_image(name=f\"matrix/wa1,wa2/{iterNum}\",step=epoch,data=[model.wa_1.detach().cpu(),model.wa_2.detach().cpu()])\n",
    "                            show_image(name=f\"matrix/wb1,wb2/{iterNum}\",step=epoch,data=[model.wb_1.detach().cpu(),model.wb_2.detach().cpu()]) \n",
    "                        if  section['pos_emb']!='none':\n",
    "                            show_image(name=f\"matrix/pos/{iterNum}\",step=epoch,data=[model.pos.detach().cpu()])\n",
    "\n",
    "                        # show_image(name=f\"matrix/input_tokens/{iterNum}\",step=epoch,data=[model.input_tokens])\n",
    "                        # show_image(name=f\"matrix/output_tokens/{iterNum}\",step=epoch,data=[model.output_tokens])\n",
    "                        # show_image(name=f\"matrix/normed_tokens/{iterNum}\",step=epoch,data=[model.normed_tokens])\n",
    "                        # show_image(name=f\"matrix/input_output_norm/{iterNum}\",step=epoch,data=[model.input_tokens.detach().cpu(),model.output_tokens.detach().cpu(),model.normed_tokens.detach().cpu()])\n",
    "                    # save the parameters in network\n",
    "                    if oa > BestAcc:\n",
    "                            BestAcc = oa\n",
    "                            fileName = save_checkpoint(checkpointDatasetPath,configName+\"_saveCheckpoint.pkl\",model)\n",
    "                            bestmodel = model\n",
    "                    model.train()  \n",
    "                    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        Train_time = end - start\n",
    "\n",
    "        # # load the saved parameters\n",
    "        # if 'TEST' in configName:\n",
    "        #     #调试不必打印保存\n",
    "        #     return\n",
    "\n",
    "        model.load_state_dict(torch.load(fileName))\n",
    "        os.remove(fileName)\n",
    "\n",
    "        model.eval()\n",
    "        confusion, oa, each_acc, aa, kappa = reports(testLoader,model,datasetConfig.classNum,datasetName)\n",
    "        KAPPA.append(kappa)\n",
    "        OA.append(oa)\n",
    "        AA.append(aa)\n",
    "        ELEMENT_ACC[iterNum, :] = each_acc\n",
    "        torch.save(model, checkpointPath / datasetName / f'best_model_{checkpointName}_OA={oa}_AA={aa}_Iter={iterNum}_{current_time}_{datasetName}.pt')\n",
    "        recordExcel(oa,aa,kappa,checkpointName,ELEMENT_ACC)\n",
    "\n",
    "    print(\"----------\" + datasetName + \" Training Finished -----------\")\n",
    "    record_output(OA, AA, KAPPA, ELEMENT_ACC,resultPath / f\"{current_time}_{checkpointName}_{datasetName}\")\n",
    "\n",
    "    writer.add_hparams(\n",
    "       params,\n",
    "        {\n",
    "            \"max_OA\":max(OA),\n",
    "            \"max_AA\":max(AA),\n",
    "            \"max_kappa\":max(KAPPA),\n",
    "        })\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    return checkpointPath / datasetName / f'best_model_{checkpointName}_OA={oa}_AA={aa}_Iter={iterNum}_{current_time}_{datasetName}.pt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def padding_hsi( input_normalize,mode='mirror'):\n",
    "    patch = patchsize\n",
    "    height, width, band = input_normalize.shape\n",
    "    padding = patch // 2\n",
    "    mirror_hsi = np.zeros((height + 2 * padding, width + 2 * padding, band), dtype=float)  # padding后的图 上下左右各加padding\n",
    "\n",
    "    mirror_hsi[padding:(padding + height), padding:(padding + width), :] = input_normalize  # 中间用原图初始化\n",
    "    if mode == 'mirror':\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[padding:(height + padding), i, :] = input_normalize[:, padding - i - 1, :]\n",
    "\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[padding:(height + padding), width + padding + i, :] = input_normalize[:, width - 1 - i, :]\n",
    "\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[i, :, :] = mirror_hsi[padding * 2 - i - 1, :, :]\n",
    "\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[height + padding + i, :, :] = mirror_hsi[height + padding - 1 - i, :, :]\n",
    "    elif mode == 'zero':\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[padding:(height + padding), i, :] = 0\n",
    "\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[padding:(height + padding), width + padding + i, :] = 0\n",
    "\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[i, :, :] = 0\n",
    "\n",
    "        for i in range(padding):\n",
    "            mirror_hsi[height + padding + i, :, :] = 0\n",
    "\n",
    "    print(\"**************************************************\")\n",
    "    print(\"patch is : {}\".format(patch))\n",
    "    print(\"mirror_image shape : [{0},{1},{2}]\".format(mirror_hsi.shape[0], mirror_hsi.shape[1], mirror_hsi.shape[2]))\n",
    "    print(\"**************************************************\")\n",
    "    return mirror_hsi\n",
    "def gain_neighborhood_pixel(mirror_image, point, i, patch=5):\n",
    "    x = point[i, 0]\n",
    "    y = point[i, 1]\n",
    "    temp_image = mirror_image[x:(x + patch), y:(y + patch), :]\n",
    "    return temp_image\n",
    "def true_data(mirror_image, band,  true_point, patch=11):\n",
    "    x_true = np.zeros((true_point.shape[0], patch, patch, band), dtype=float)  #\n",
    "    for k in range(true_point.shape[0]):\n",
    "        x_true[k, :, :, :] = gain_neighborhood_pixel(mirror_image, true_point, k, patch)\n",
    "    x_true = torch.from_numpy(x_true).to(torch.float32)\n",
    "    x_true = x_true.permute(0,3,1,2)\n",
    "    x_true = x_true.reshape(x_true.shape[0],x_true.shape[1],-1).to(torch.float32)\n",
    "    return  x_true\n",
    "def pred_all(model):\n",
    "    # load mat\n",
    "    def load_mat(modalName):\n",
    "        path_img = parent_directory / \"dataset\" / \"img\" / datasetName / \"{}_norm.mat\".format(modalName)\n",
    "        data1 = io.loadmat(path_img)\n",
    "\n",
    "        data1 = data1['Data']\n",
    "        h,w,c = data1.shape\n",
    "\n",
    "\n",
    "        data1 = padding_hsi(data1,mode='mirror')\n",
    "        return data1,h,w,c\n",
    "    data1,h,w,c1 = load_mat(modalName=modalName1)\n",
    "    data2,h,w,c2 = load_mat(modalName=modalName2)\n",
    "    test_batch=int(section['test_batch'])\n",
    "    number = h * w // test_batch\n",
    "    total_pos_true = np.array([[i,j] for i in range(h) for j in range(w)])\n",
    "\n",
    "    pred_all = np.empty((h*w, 1), dtype='float64')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(number):\n",
    "            temp_pos = total_pos_true[i * test_batch:(i + 1) * test_batch,:]\n",
    "            temp_data1 = true_data(data1, c1, temp_pos, patch=patchsize,).cuda()\n",
    "            temp_data2 = true_data(data2, c2, temp_pos, patch=patchsize,).cuda()\n",
    "            temp2 = model(temp_data1,temp_data2)\n",
    "            del temp_data1,temp_data2\n",
    "            temp3 = torch.max(temp2, 1)[1].squeeze()\n",
    "            del temp2\n",
    "            pred_all[i * test_batch:(i + 1) * test_batch, 0] = temp3.cpu()\n",
    "            del temp3\n",
    "\n",
    "        if (i + 1) * test_batch < h * w:\n",
    "            temp_pos = total_pos_true[(i + 1) * test_batch:h * w, :]\n",
    "            temp_data1 = true_data(data1, c1, temp_pos, patch=patchsize,).cuda()\n",
    "            temp_data2 = true_data(data2, c2, temp_pos, patch=patchsize,).cuda()\n",
    "            temp2 = model(temp_data1,temp_data2)\n",
    "            del temp_data1,temp_data2\n",
    "            temp3 = torch.max(temp2, 1)[1].squeeze()\n",
    "            del temp2\n",
    "            pred_all[(i + 1) * test_batch:h * w, 0] = temp3.cpu()\n",
    "            del temp3\n",
    "\n",
    "        pred_all = np.reshape(pred_all, (h, w)) + 1\n",
    "    return pred_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def draw_img(model):\n",
    "    prediction_matrix = pred_all(model)\n",
    "    colormap = {\n",
    "\n",
    "    \"MUUFL\": colors.ListedColormap(\n",
    "          [\"#0000cd\",\"#0008ff\",\"#004dff\",\"#0091ff\",\"#00d4ff\",\"#29ffce\",\"#60ff97\",\"#97ff60\",\"#ceff29\",\"#ffe600\",\"#ffa700\"]),\n",
    "        \"Trento\": colors.ListedColormap(\n",
    "           [\"#0000cd\",\"#0008ff\",\"#004dff\",\"#0091ff\",\"#00d4ff\",\"#29ffce\"]),\n",
    "        \"Augsburg\": colors.ListedColormap(\n",
    "            [\"#0000cd\",\"#0008ff\",\"#004dff\",\"#0091ff\",\"#00d4ff\",\"#29ffce\",\"#60ff97\"]),\n",
    "    }\n",
    "    # savemat(f'{args.dataset}_matrix.mat', {'Data': prediction_matrix})\n",
    "    plt.subplot(1, 1, 1)\n",
    "    print(type(prediction_matrix))\n",
    "    plt.imshow(prediction_matrix, cmap=colormap[datasetName])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xticks(alpha=0)\n",
    "    plt.yticks(alpha=0)\n",
    "    plt.axis('off')\n",
    "    plt.tick_params(axis='x', width=0)\n",
    "    plt.tick_params(axis='y', width=0)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.legend()\n",
    "    imgDatasetPath = imgPath / datasetName\n",
    "    imgDatasetPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fileName = imgDatasetPath / \"{}_{}.png\".format(section[\"network\"],configName)\n",
    "    plt.savefig(fileName, bbox_inches='tight', dpi=1000,pad_inches=0)\n",
    "    io.savemat(imgDatasetPath / \"{}_{}.mat\".format(section[\"network\"],configName),{'Data':prediction_matrix})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     model = torch.load_state_dict(torch.load(fileName))\n",
    "\n",
    "#             model.eval()\n",
    "#             confusion, oa, each_acc, aa, kappa = reports(testLoader,model,datasetConfig.classNum,datasetName)\n",
    "#             KAPPA.append(kappa)\n",
    "#             OA.append(oa)\n",
    "#             AA.append(aa)\n",
    "#             ELEMENT_ACC[iterNum, :] = each_acc\n",
    "#             torch.save(model, checkpointPath / datasetName / f'best_model_{checkpointName}_OA={oa}_AA={aa}_Iter={iterNum}_{current_time}_{datasetName}.pt')\n",
    "#             recordExcel(oa,aa,kappa,checkpointName)\n",
    "\n",
    "#         print(\"----------\" + datasetName + \" Training Finished -----------\")\n",
    "#         record_output(OA, AA, KAPPA, ELEMENT_ACC,resultPath / f\"{current_time}_{checkpointName}_{datasetName}\")\n",
    "        \n",
    "#         writer.add_hparams(\n",
    "#            params,\n",
    "#             {\n",
    "#                 \"max_OA\":max(OA),\n",
    "#                 \"max_AA\":max(AA),\n",
    "#                 \"max_kappa\":max(KAPPA),\n",
    "#             })\n",
    "#         writer.flush()\n",
    "#         writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# def getParams(datasetName,modelName,patchsize,band1,band2,classNum):# get patams of HCT\\MFT\\ExViT\\Ours\n",
    "\n",
    "#     model = getModel(modelName,band1,band2,classNum,patchsize).cuda()\n",
    "#     model.eval()\n",
    "#     # print(((64,datasetConfig.bandNum1, patchsize**2), (64, datasetConfig.bandNum2,patchsize**2)))\n",
    "#     print(model)\n",
    "#     summary(model, [(band1, patchsize**2), ( band2,patchsize**2)],batch_size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummaryX import summary\n",
    "def getParams(datasetName,modelName,patchsize,band1,band2,classNum):\n",
    "    model = getModel(modelName,band1,band2,classNum,patchsize)\n",
    "    model.eval()\n",
    "    summary(model, torch.zeros((64,band1, patchsize**2)), torch.zeros((64, band2,patchsize**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSI Tr data shape = torch.Size([761, 180, 49])\n",
      "DSM Tr data shape = torch.Size([761, 1, 49])\n",
      "Tr label shape = torch.Size([761])\n",
      "Augsburg HSI DSM MFT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Conv3d             [1, 8, 9, 3, 3]   [64, 8, 172, 7, 7]                 0.66                 5.46\n",
      "1_BatchNorm3d                    [8]   [64, 8, 172, 7, 7]                 0.02                 0.00\n",
      "2_ReLU                             -   [64, 8, 172, 7, 7]                    -                    -\n",
      "3_Conv2d              [86, 64, 3, 3]       [64, 64, 7, 7]                49.60                 2.43\n",
      "4_Conv2d            [1376, 64, 1, 1]       [64, 64, 7, 7]                88.13                 4.32\n",
      "5_BatchNorm2d                   [64]       [64, 64, 7, 7]                 0.13                 0.00\n",
      "6_ReLU                             -       [64, 64, 7, 7]                    -                    -\n",
      "7_Conv2d               [1, 64, 3, 3]       [64, 64, 7, 7]                 0.64                 0.03\n",
      "8_BatchNorm2d                   [64]       [64, 64, 7, 7]                 0.13                 0.00\n",
      "9_GELU                             -       [64, 64, 7, 7]                    -                    -\n",
      "10_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "11_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "12_Linear                    [8, 64]       [64, 1, 8, 64]                 0.51                 0.00\n",
      "13_Linear                    [8, 64]       [64, 5, 8, 64]                 0.51                 0.00\n",
      "14_Linear                    [8, 64]       [64, 5, 8, 64]                 0.51                 0.00\n",
      "15_Linear                  [512, 64]          [64, 1, 64]                32.83                 0.03\n",
      "16_Dropout                         -          [64, 1, 64]                    -                    -\n",
      "17_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "18_Linear                  [64, 512]         [64, 5, 512]                33.28                 0.03\n",
      "19_GELU                            -         [64, 5, 512]                    -                    -\n",
      "20_Dropout                         -         [64, 5, 512]                    -                    -\n",
      "21_Linear                  [512, 64]          [64, 5, 64]                32.83                 0.03\n",
      "22_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "23_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "24_Linear                    [8, 64]       [64, 1, 8, 64]                 0.51                 0.00\n",
      "25_Linear                    [8, 64]       [64, 5, 8, 64]                 0.51                 0.00\n",
      "26_Linear                    [8, 64]       [64, 5, 8, 64]                 0.51                 0.00\n",
      "27_Linear                  [512, 64]          [64, 1, 64]                32.83                 0.03\n",
      "28_Dropout                         -          [64, 1, 64]                    -                    -\n",
      "29_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "30_Linear                  [64, 512]         [64, 5, 512]                33.28                 0.03\n",
      "31_GELU                            -         [64, 5, 512]                    -                    -\n",
      "32_Dropout                         -         [64, 5, 512]                    -                    -\n",
      "33_Linear                  [512, 64]          [64, 5, 64]                32.83                 0.03\n",
      "34_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "35_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "36_Linear                    [64, 7]              [64, 7]                 0.46                 0.00\n",
      "====================================================================================================\n",
      "# Params:    341.35K\n",
      "# Mult-Adds: 12.43M\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Augsburg HSI DSM HCT\n",
      "torch.Size([64, 180, 49]) torch.Size([64, 1, 49])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Conv3d             [1, 8, 3, 3, 3]   [64, 8, 178, 5, 5]                 0.22                 0.96\n",
      "1_BatchNorm3d                    [8]   [64, 8, 178, 5, 5]                 0.02                 0.00\n",
      "2_ReLU                             -   [64, 8, 178, 5, 5]                    -                    -\n",
      "3_Conv2d            [1424, 64, 3, 3]       [64, 64, 3, 3]               820.29                 7.38\n",
      "4_BatchNorm2d                   [64]       [64, 64, 3, 3]                 0.13                 0.00\n",
      "5_ReLU                             -       [64, 64, 3, 3]                    -                    -\n",
      "6_Conv2d               [1, 64, 3, 3]       [64, 64, 5, 5]                 0.64                 0.01\n",
      "7_BatchNorm2d                   [64]       [64, 64, 5, 5]                 0.13                 0.00\n",
      "8_ReLU                             -       [64, 64, 5, 5]                    -                    -\n",
      "9_Dropout                          -          [64, 5, 64]                    -                    -\n",
      "10_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "11_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "12_Linear                  [64, 192]         [64, 5, 192]                12.48                 0.01\n",
      "13_Linear                   [64, 64]          [64, 5, 64]                 4.16                 0.00\n",
      "14_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "15_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "16_Linear                    [64, 8]           [64, 5, 8]                 0.52                 0.00\n",
      "17_GELU                            -           [64, 5, 8]                    -                    -\n",
      "18_Dropout                         -           [64, 5, 8]                    -                    -\n",
      "19_Linear                    [8, 64]          [64, 5, 64]                 0.58                 0.00\n",
      "20_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "21_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "22_Linear                  [64, 192]         [64, 5, 192]                12.48                 0.01\n",
      "23_Linear                   [64, 64]          [64, 5, 64]                 4.16                 0.00\n",
      "24_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "25_LayerNorm                    [64]          [64, 5, 64]                 0.13                 0.00\n",
      "26_Linear                    [64, 8]           [64, 5, 8]                 0.52                 0.00\n",
      "27_GELU                            -           [64, 5, 8]                    -                    -\n",
      "28_Dropout                         -           [64, 5, 8]                    -                    -\n",
      "29_Linear                    [8, 64]          [64, 5, 64]                 0.58                 0.00\n",
      "30_Dropout                         -          [64, 5, 64]                    -                    -\n",
      "31_Identity                        -          [64, 1, 64]                    -                    -\n",
      "32_LayerNorm                    [64]          [64, 1, 64]                 0.13                 0.00\n",
      "33_Linear                  [64, 512]         [64, 1, 512]                32.77                 0.03\n",
      "34_Linear                 [64, 1024]        [64, 5, 1024]                65.54                 0.07\n",
      "35_Softmax                         -        [64, 8, 1, 5]                    -                    -\n",
      "36_Dropout                         -        [64, 8, 1, 5]                    -                    -\n",
      "37_Linear                  [512, 64]          [64, 1, 64]                32.83                 0.03\n",
      "38_Dropout                         -          [64, 1, 64]                    -                    -\n",
      "39_Identity                        -          [64, 1, 64]                    -                    -\n",
      "40_Identity                        -          [64, 1, 64]                    -                    -\n",
      "41_LayerNorm                    [64]          [64, 1, 64]                 0.13                 0.00\n",
      "42_Linear                  [64, 512]         [64, 1, 512]                32.77                 0.03\n",
      "43_Linear                 [64, 1024]        [64, 5, 1024]                65.54                 0.07\n",
      "44_Softmax                         -        [64, 8, 1, 5]                    -                    -\n",
      "45_Dropout                         -        [64, 8, 1, 5]                    -                    -\n",
      "46_Linear                  [512, 64]          [64, 1, 64]                32.83                 0.03\n",
      "47_Dropout                         -          [64, 1, 64]                    -                    -\n",
      "48_Identity                        -          [64, 1, 64]                    -                    -\n",
      "49_LayerNorm                    [64]             [64, 64]                 0.13                 0.00\n",
      "50_Linear                    [64, 7]              [64, 7]                 0.46                 0.00\n",
      "51_LayerNorm                    [64]             [64, 64]          (recursive)                 0.00\n",
      "52_Linear                    [64, 7]              [64, 7]          (recursive)                 0.00\n",
      "====================================================================================================\n",
      "# Params:    1,120.52K\n",
      "# Mult-Adds: 8.66M\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Augsburg HSI DSM ExViT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Conv2d              [1, 180, 3, 3]      [64, 180, 7, 7]                 1.80                 0.08\n",
      "1_Conv2d             [180, 16, 1, 1]       [64, 16, 7, 7]                 2.90                 0.14\n",
      "2_BatchNorm2d                   [16]       [64, 16, 7, 7]                 0.03                 0.00\n",
      "3_GELU                             -       [64, 16, 7, 7]                    -                    -\n",
      "4_Conv2d               [1, 16, 3, 3]       [64, 16, 7, 7]                 0.16                 0.01\n",
      "5_Conv2d              [16, 32, 1, 1]       [64, 32, 7, 7]                 0.54                 0.03\n",
      "6_BatchNorm2d                   [32]       [64, 32, 7, 7]                 0.06                 0.00\n",
      "7_GELU                             -       [64, 32, 7, 7]                    -                    -\n",
      "8_Conv2d               [1, 32, 3, 3]       [64, 32, 7, 7]                 0.32                 0.01\n",
      "9_Conv2d              [32, 64, 1, 1]       [64, 64, 7, 7]                 2.11                 0.10\n",
      "10_BatchNorm2d                  [64]       [64, 64, 7, 7]                 0.13                 0.00\n",
      "11_GELU                            -       [64, 64, 7, 7]                    -                    -\n",
      "12_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "13_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "14_Conv2d               [1, 1, 3, 3]        [64, 1, 7, 7]                 0.01                 0.00\n",
      "15_Conv2d              [1, 16, 1, 1]       [64, 16, 7, 7]                 0.03                 0.00\n",
      "16_BatchNorm2d                  [16]       [64, 16, 7, 7]                 0.03                 0.00\n",
      "17_GELU                            -       [64, 16, 7, 7]                    -                    -\n",
      "18_Conv2d              [1, 16, 3, 3]       [64, 16, 7, 7]                 0.16                 0.01\n",
      "19_Conv2d             [16, 32, 1, 1]       [64, 32, 7, 7]                 0.54                 0.03\n",
      "20_BatchNorm2d                  [32]       [64, 32, 7, 7]                 0.06                 0.00\n",
      "21_GELU                            -       [64, 32, 7, 7]                    -                    -\n",
      "22_Conv2d              [1, 32, 3, 3]       [64, 32, 7, 7]                 0.32                 0.01\n",
      "23_Conv2d             [32, 64, 1, 1]       [64, 64, 7, 7]                 2.11                 0.10\n",
      "24_BatchNorm2d                  [64]       [64, 64, 7, 7]                 0.13                 0.00\n",
      "25_GELU                            -       [64, 64, 7, 7]                    -                    -\n",
      "26_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "27_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "28_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "29_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "30_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "31_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "32_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "33_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "34_GELU                            -         [64, 49, 16]                    -                    -\n",
      "35_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "36_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "37_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "38_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "39_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "40_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "41_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "42_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "43_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "44_GELU                            -         [64, 49, 16]                    -                    -\n",
      "45_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "46_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "47_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "48_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "49_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "50_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "51_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "52_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "53_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "54_GELU                            -         [64, 49, 16]                    -                    -\n",
      "55_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "56_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "57_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "58_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "59_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "60_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "61_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "62_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "63_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "64_GELU                            -         [64, 49, 16]                    -                    -\n",
      "65_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "66_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "67_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "68_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "69_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "70_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "71_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "72_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "73_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "74_GELU                            -         [64, 49, 16]                    -                    -\n",
      "75_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "76_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "77_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "78_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "79_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "80_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "81_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "82_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "83_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "84_GELU                            -         [64, 49, 16]                    -                    -\n",
      "85_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "86_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "87_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "88_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "89_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "90_Linear                   [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "91_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "92_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "93_Linear                   [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "94_GELU                            -         [64, 49, 16]                    -                    -\n",
      "95_Dropout                         -         [64, 49, 16]                    -                    -\n",
      "96_Linear                   [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "97_Dropout                         -         [64, 49, 64]                    -                    -\n",
      "98_LayerNorm                    [64]         [64, 49, 64]                 0.13                 0.00\n",
      "99_Linear                  [64, 192]        [64, 49, 192]                12.29                 0.01\n",
      "100_Linear                  [64, 64]         [64, 49, 64]                 4.16                 0.00\n",
      "101_Dropout                        -         [64, 49, 64]                    -                    -\n",
      "102_LayerNorm                   [64]         [64, 49, 64]                 0.13                 0.00\n",
      "103_Linear                  [64, 16]         [64, 49, 16]                 1.04                 0.00\n",
      "104_GELU                           -         [64, 49, 16]                    -                    -\n",
      "105_Dropout                        -         [64, 49, 16]                    -                    -\n",
      "106_Linear                  [16, 64]         [64, 49, 64]                 1.09                 0.00\n",
      "107_Dropout                        -         [64, 49, 64]                    -                    -\n",
      "108_LayerNorm                   [64]         [64, 98, 64]                 0.13                 0.00\n",
      "109_Linear                 [64, 192]        [64, 98, 192]                12.29                 0.01\n",
      "110_Linear                  [64, 64]         [64, 98, 64]                 4.16                 0.00\n",
      "111_Dropout                        -         [64, 98, 64]                    -                    -\n",
      "112_LayerNorm                   [64]         [64, 98, 64]                 0.13                 0.00\n",
      "113_Linear                  [64, 16]         [64, 98, 16]                 1.04                 0.00\n",
      "114_GELU                           -         [64, 98, 16]                    -                    -\n",
      "115_Dropout                        -         [64, 98, 16]                    -                    -\n",
      "116_Linear                  [16, 64]         [64, 98, 64]                 1.09                 0.00\n",
      "117_Dropout                        -         [64, 98, 64]                    -                    -\n",
      "118_LayerNorm                   [64]         [64, 98, 64]                 0.13                 0.00\n",
      "119_Linear                 [64, 192]        [64, 98, 192]                12.29                 0.01\n",
      "120_Linear                  [64, 64]         [64, 98, 64]                 4.16                 0.00\n",
      "121_Dropout                        -         [64, 98, 64]                    -                    -\n",
      "122_LayerNorm                   [64]         [64, 98, 64]                 0.13                 0.00\n",
      "123_Linear                  [64, 16]         [64, 98, 16]                 1.04                 0.00\n",
      "124_GELU                           -         [64, 98, 16]                    -                    -\n",
      "125_Dropout                        -         [64, 98, 16]                    -                    -\n",
      "126_Linear                  [16, 64]         [64, 98, 64]                 1.09                 0.00\n",
      "127_Dropout                        -         [64, 98, 64]                    -                    -\n",
      "128_LayerNorm                   [64]         [64, 98, 64]                 0.13                 0.00\n",
      "129_Linear                   [64, 1]          [64, 98, 1]                 0.07                 0.00\n",
      "130_Softmax                        -          [64, 98, 1]                    -                    -\n",
      "131_LayerNorm                   [64]             [64, 64]                 0.13                 0.00\n",
      "132_Linear                   [64, 7]              [64, 7]                 0.46                 0.00\n",
      "====================================================================================================\n",
      "# Params:    208.87K\n",
      "# Mult-Adds: 0.71M\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Augsburg HSI DSM Minato\n",
      "torch.Size([64, 180, 49]) torch.Size([64, 1, 49])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Conv3d             [1, 8, 3, 3, 3]   [64, 8, 178, 7, 7]                 0.22                 1.88\n",
      "1_BatchNorm3d                    [8]   [64, 8, 178, 7, 7]                 0.02                 0.00\n",
      "2_ReLU                             -   [64, 8, 178, 7, 7]                    -                    -\n",
      "3_Conv2d            [1424, 64, 3, 3]       [64, 64, 5, 5]               820.29                20.51\n",
      "4_BatchNorm2d                   [64]       [64, 64, 5, 5]                 0.13                 0.00\n",
      "5_ReLU                             -       [64, 64, 5, 5]                    -                    -\n",
      "6_Conv2d               [1, 64, 3, 3]       [64, 64, 5, 5]                 0.64                 0.01\n",
      "7_BatchNorm2d                   [64]       [64, 64, 5, 5]                 0.13                 0.00\n",
      "8_ReLU                             -       [64, 64, 5, 5]                    -                    -\n",
      "9_Softmax                          -         [64, 25, 64]                    -                    -\n",
      "10_Softmax                         -         [64, 25, 64]                    -                    -\n",
      "11_Dropout                         -        [64, 256, 64]                    -                    -\n",
      "12_LayerNorm                    [64]        [64, 256, 64]                 0.13                 0.00\n",
      "13_AdaptiveAvgPool1d                    -          [64, 64, 1]                    -                    -\n",
      "14_LayerNorm                    [64]             [64, 64]                 0.13                 0.00\n",
      "15_Linear                    [64, 7]              [64, 7]                 0.46                 0.00\n",
      "====================================================================================================\n",
      "# Params:    822.13K\n",
      "# Mult-Adds: 22.40M\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if section['do']=='train' or section['do']=='trainAndDraw' :\n",
    "    filename = train()\n",
    "    print(filename)\n",
    "    model = torch.load(filename)\n",
    "    if section['do']=='trainAndDraw':\n",
    "        # print(draw)\n",
    "        draw_img(model)\n",
    "elif section['do']=='draw':\n",
    "    model = torch.load(checkpointDatasetPath / section['load_model'])\n",
    "    draw_img(model)\n",
    "elif section['do']=='params':\n",
    "    datasetConfig = DatasetConfig(datasetName)\n",
    "    trainLoader = datasetConfig.getTrainLoader(type = \"Tr\",batchsize = batchsize)\n",
    "    modelNames = [\"MFT\",\"HCT\",\"ExViT\",\"Minato\"]\n",
    "    for m in modelNames:\n",
    "        print(datasetName,modalName1,modalName2,m)\n",
    "        getParams(datasetName=datasetName,modelName=m,patchsize=patchsize,band1=datasetConfig.bandNum1,band2=datasetConfig.bandNum2,classNum=datasetConfig.classNum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.vision.all import *\n",
    "# # from migrating_pytorch import *\n",
    "# from fastai.optimizer import OptimWrapper\n",
    "# import fastai.callback.schedule\n",
    "# from fastai.metrics import accuracy\n",
    "# from functools import partial\n",
    "# def fastAi():\n",
    "#     datasetConfig = DatasetConfig(datasetName)\n",
    "#\n",
    "#     trainLoader = datasetConfig.getTrainLoader(type = \"Tr\",batchsize = batchsize)\n",
    "#     testLoader = datasetConfig.getTrainLoader(type = \"Te\",batchsize = testSizeNumber)\n",
    "#     KAPPA = []\n",
    "#     OA = []\n",
    "#     AA = []\n",
    "#     ELEMENT_ACC = np.zeros((1, datasetConfig.classNum))\n",
    "#\n",
    "#\n",
    "#     model = Minato(datasetConfig.classNum, HSIOnly,datasetConfig.shape1,datasetConfig.shape2).cuda() #TODO: change modal here\n",
    "#     checkpointDatasetPath = checkpointPath / datasetName\n",
    "#     checkpointDatasetPath.mkdir(parents=True, exist_ok=True)\n",
    "#     checkpointDatasetPath = checkpointDatasetPath / \"saveCheckpoint\"\n",
    "#     # optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-3)\n",
    "#     opt_func = partial(OptimWrapper, opt=torch.optim.Adam)\n",
    "#     loss_func = nn.CrossEntropyLoss()  # the target label is not one-hotted\n",
    "#     # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "#\n",
    "#     data = DataLoaders(trainLoader, testLoader)\n",
    "#     learn = Learner(data, model, loss_func=loss_func, opt_func=opt_func, metrics=accuracy)\n",
    "#\n",
    "#     # train\n",
    "#     # learn.fit_one_cycle(n_epoch=1, lr_max=1e-2)\n",
    "#     # learn.lr_find()\n",
    "#     lrs = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "#     print(lrs)\n",
    "#     # print('Learning rate with the minimum loss:', lr_min)\n",
    "#     # print('Learning rate with the steepest gradient:', lr_steep)\n",
    "# #     learn.save(checkpointDatasetPath, with_opt=False)\n",
    "#\n",
    "# #     model.load_state_dict(torch.load(str(checkpointDatasetPath)+\".pth\"))\n",
    "# #     model.eval()\n",
    "# #     confusion, oa, each_acc, aa, kappa = reports(testLoader,model,datasetConfig.classNum,datasetName)\n",
    "# #     KAPPA.append(kappa)\n",
    "# #     OA.append(oa)\n",
    "# #     AA.append(aa)\n",
    "# #     ELEMENT_ACC[0, :] = each_acc\n",
    "# #     torch.save(model, checkpointPath / datasetName / f'best_model_{checkpointName}_OA={oa}_AA={aa}_Iter={0}_{current_time}_{datasetName}.pt')\n",
    "# #     recordExcel(oa,aa,kappa,checkpointName)\n",
    "# #     print(\"----------\" + datasetName + \" Training Finished -----------\")\n",
    "# #     record_output(OA, AA, KAPPA, ELEMENT_ACC,resultPath / f\"{current_time}_{checkpointName}_{datasetName}\")\n",
    "# #     writer.add_hparams(\n",
    "# #        params,\n",
    "# #         {\n",
    "# #             \"max_OA\":max(OA),\n",
    "# #             \"max_AA\":max(AA),\n",
    "# #             \"max_kappa\":max(KAPPA),\n",
    "# #         })\n",
    "# #     writer.flush()\n",
    "# #     writer.close()\n",
    "# if section['do']=='fastai':\n",
    "#     fastAi()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
